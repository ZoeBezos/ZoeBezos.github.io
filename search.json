[{"title":"Certificate","url":"/2023/06/10/6-10-certificate/","content":"\n- [ ] 软考-高级：抵税\n- [ ] 教师资格证\n- [ ] 健身教练\n- [ ] 游泳教练\n- [ ] 公共营养师\n- [ ] 健康管理师\n- [ ] 心理咨询师\n- [ ] 注册会计师\n- [ ] 翻译证书\n- [ ] 全媒体运营师\n- [ ] 整理收纳师\n- [ ] 韩语\n- [ ] 日语\n","tags":["preview"],"categories":["interest-list"]},{"title":"Software Development Ideas","url":"/2023/06/10/6-10-software-development-ideas/","content":"\n[toc]\n\n# 生态性质的“管家式”软件\n\n- 多方面的复盘\n  - 时间复盘\n    - 睡眠时间，起床时间，入睡时间\n    - 应用软件使用时间\n    - 学习、工作、健身、读书时间\n  - 金钱复盘\n\n- 多维度的复盘\n  - 按照日、周、月、季度、年复盘\n\n- 事务管理\n\n  | 睡眠 | 8h   |\n  | ---- | ---- |\n  | 学习 | 8h   |\n  | 健身 | 3h   |\n  | 读书 | 1h   |\n  | 其他 | 4h   |\n\n\n\n可以参考\n\n<img src=\"6-10-software-development-ideas/image-20230610225907531.png\" alt=\"image-20230610225907531\" style=\"zoom:33%;\" />\n\n<img src=\"6-10-software-development-ideas/image-20230610225938260.png\" alt=\"image-20230610225938260\" style=\"zoom:33%;\" />\n\n# .md文件转化为毕业论文\n\n## 用处：\n\n按照标题的级数自动转换格式\n\n## 人群\n\n写毕业论文的年轻人\n\n# 智能衣橱\n\n## 用处：\n\n​\t可以方便使用胶囊衣柜的人，每天穿干净和前天不同的衣服\n\n## 人群：\n\n​\t专攻性价比，瞄准刚毕业的年轻人\n","tags":["preview"],"categories":["interest-list"]},{"title":"Interest List","url":"/2023/06/10/6-10-interest-list/","content":"\n# 原因\n\n我打算写下兴趣清单，这样空下来的时候，我就能知道我可以做什么了。\n\n# 事项\n\n- [ ] 学做饭\n- [ ] 看《never had I ever》第四季\n- [ ] 学习改善体态\n- [ ] 学习表情管理\n- [ ] 学习眼神管理\n- [ ] 学习声音管理\n- [ ] 练习演讲\n- [ ] \n","tags":["preview"],"categories":["interest-list"]},{"title":"Tips 1.0","url":"/2023/06/09/6-9-tips/","content":"\n[toc]\n\n# 生活作息：\n\n- [ ] 保持早睡\n- [ ] 不刷朋友圈，也不发朋友圈\n- [ ] 吃饭时不看屏幕\n- [ ] 睡前三件套：泡脚，冥想，看书\n- [ ] 早起时，想想令自己幸福的事：家人安定生活，工作取得成就\n- [ ] 每天健身\n- [ ] \n\n# 学习/工作：\n\n- [ ] 每天、每周、每月、每季度、每年复盘，可以逐渐修正自己的缺点\n- [ ] 拖延可以毁掉一个人\n- [ ] 感到焦虑说明不喜欢现在的状态，却不肯走出舒适圈。破局的方法是，尽快行动\n- [ ] 不要相信眼睛看的一切，要用心体会。不要在嘴上追求成功，要用手创造\n- [ ] 要相信相信的力量\n- [ ] 将最重要的任务放在早上\n\n# 社交：\n\n- [ ] 多向厉害的人学习\n- [ ] 少说评价性的话，用事实陈述语句代替\n- [ ] 不要被不好意思耽误人生\n- [ ] 凡事永远提前15分钟\n- [ ] 脸皮厚\n- [ ] 不活在别人眼里，只关注自己目标\n- [ ] 不把“没钱”“好穷”放在嘴边\n- [ ] 记仇，永远不给一个人害你两次的机会\n\n\n\n# 财务：\n\n- [ ] 超前消费是失去自由的前兆\n- [ ] 经济独立的真正意义是生活遭遇沉重打击的时候，账户里的存款\n\n# 不开心的时候可以做：\n\n- [ ] 收拾房间\n- [ ] 打扫卫生\n- [ ] 整理布局：手机桌面，电脑桌面，平板桌面、浏览器桌面\n- [ ] 整理收藏的东西：b站，小红书，知乎，微信\n- [ ] 清理联系人\n","tags":["review"],"categories":["tips"]},{"title":"Book List","url":"/2023/06/09/6-9-book-list/","content":"\n[toc]\n\n# 小说\n\n- [ ] 三体\n- [ ] 白鹿原\n- [ ] 使女的故事\n- [ ] 美丽新世界\n- [ ] 1984\n\n\n\n# 传记\n\n- [ ] 巴菲特传\n- [ ] 稻盛和夫自传\n- [ ] 钱钟书传\n- [ ] 成为波伏娃\n- [ ] 苏格拉底传\n\n# 逻辑类\n\n- [ ] 我为什么不结婚\n- [ ] 厌女\n- [ ] 君主论\n- [ ] \n\n# 医学健康类\n\n- [ ] 护照\n- [ ] 施瓦辛格健身全书\n- [ ] 皮肤的秘密\n\n# 人类/心理/社会类\n\n- [ ] 心理学与生活\n- [ ] 跨越边界的社区\n- [ ] 乌合之众\n- [ ] 人性心理学\n- [ ] 乡土中国\n\n# 法律类\n\n- [ ] 刑法学讲义\n- [ ] 洞学奇案\n\n# 财经类\n\n- [ ] 经济学原理\n- [ ] 货币金融学\n- [ ] 聪明的投资者\n\n# 计算机发展史\n\n- [ ] 浪潮之巅\n\n# 历史类\n\n- [ ] 明朝那些事\n- [ ] 中国通史\n- [ ] 全球通史\n- [ ] 极简宇宙史\n\n# 哲学\n\n- [ ] 大问题 简明哲学导论\n- [ ] 理想国\n- [ ] 沉思录\n- [ ] 哲学的历程\n- [ ] 认识你自己\n- [ ] 纯粹理性批判\n- [ ] 存在与虚无\n- [ ] 逻辑与哲学\n- [ ] 你的第一本哲学书\n\n# 艺术\n\n- [ ] 艺术与生活\n\n# 设计\n\n- [ ] 给大家看的设计书\n\n# 摄影\n\n- [ ] 美国纽约摄影学院摄影教材\n\n# 电影\n\n- [ ] 认识电影\n\n# 传播学\n\n- [ ] 舆论\n\n# 考古\n\n- [ ] 考古学与史前文明\n\n# 生物学\n\n- [ ] 所罗门王的指环\n\n# 死亡\n\n- [ ] 安乐死现场\n\n# 俄罗斯文学\n\n- [ ] 白夜\n- [ ] 罪与罚\n- [ ] 白痴\n- [ ] 卡拉马佐夫兄弟\n- [ ] 少年\n- [ ] 群魔\n- [ ] 地下室手记\n\n# 人生意义\n\n- [ ] 编周记\n\n# 两性婚姻\n\n- [ ] 爱的艺术\n\n# 情绪管理\n\n- [ ] 伯恩斯新情绪疗法\n\n# 自然\n\n- [ ] 看不见的自然\n\n# 宗教\n\n- [ ] 佛教的见地和修道\n\n","tags":["hobby"],"categories":["reading"]},{"title":"Video Review 1.0","url":"/2023/06/08/6-8-vedio-review/","content":"\n[toc]\n\n# 前言\n\n今年没有看多少书——痴迷于先获得kindle scribe&买不起。\n\n但是收藏的好的视频，也是有意义的，所以打算定期整理刷过的视频。\n\n# 哔哩哔哩\n\n## [一个看上去矛盾的心理悖论，却能减轻焦虑]([](6-8-vedio-review/1.mp4))\n\n**文案：**\n\n​\t\t心理学里有个反常识的观点，**所有的改变都建立在允许不改变的基础之上**，这句话似乎有些矛盾，但当你真正领悟后，焦虑感就会大大减轻。\n\n​\t\t我们很多人追求的改变是是建立在否定原来自己的条件下，例如你憎恨自己的无知，拼命学习知识，不接受自己的身材，努力去健身，讨厌自己的长相，拼命学习化妆，这些努力都意味着你对自己的爱是有条件的，只有变得有学识了，你才爱自己，变成了A4腰了，你才爱自己，变成白又瘦了，你才爱自己——但其实改变和不改变都是我们的一部分，我们需要让这两部分达成和解，而不是服务一半否定另一半，我们有权利以任何一种方式活着，我们可以去追求幽默的谈吐，但也要允许自己代办。我们可以去追求明亮的外表，但也要允许自己普通，我们可以去追求杀伐果断，但也要允许自己笨拙，对自己无条件的坦诚才是最高级别的勇敢。当然我们也可以允许自己不勇敢，爱自己的前提就是对自己无条件的接纳，真正的强大就是允许自己以任何方式活着。\n\n\n\n**我的感受：**\n\n​\t\t对于他提出的观点，我需要先想清楚以下的问题：\n\n​\t\ta. 我爱一个人需要条件吗？\n\n​\t\tb. 我爱过人吗？\n\n​\t\tc. 我被爱过吗？\n\n​\t\td. 怎么才算爱一个人？\n\n![1](6-8-vedio-review/1.png)\n\n​\t\t根据上面的ChatGPT的回答，d问题得到解决了。\n\n​\t\t以上几条我的父母和弟弟都是符合的，所以他们一定是爱我的，c问题也得到解决。\n\n​\t\t另外两个问题则很复杂，因为主体是我自己，而我自己 totally messed up。\n\n​\t\t我回忆起一幢往事：在初中等班车回家的时候，遇到一个认识的姐姐，她说自己本来在上大学，有一年她的父亲生病了，她专门花了一年时间来照顾父亲。当时我意气风发，在我们的小破学校里面算是风云人物——全学校都相信我会考上很好的高中，我也很在意我的前程。但是我没想过怎么在家人和自己的前程做出果断的抉择。彼时，母亲已经过世3年了，我亲身经历了失去亲人的痛苦。我问我自己如果同样的事情发生在我身上，我会做出同样的选择吗？我忘记了当时我对自己的回答。但显然，无论是哪个选择，我都没有做出果断的选择。而这个犹豫让我意识到，我一定不是爱父亲超过自己。\n\n​\t\t那次经历得出的结论并不是01结果（爱或者不爱），而是一个二者比较结果（爱自己和爱别人的程度比较）\n\n​\t\t那换一种推测方法——看看d问题的答案决定：\n\n- [x] 我在有一定的物质储蓄之后，一定是愿意付出时间精力让他们快乐的\n\n- [x] 同上\n\n- [x] 同上\n\n- [x] 同上，在获得自由之后，我愿意让家人们学习新事物，享受人生\n\n- [x] 符合\n\n- [x] 符合\n\n  ​\t以上测评令我明白——我也是爱我家人的，但十分担心经济情况。\n\n  ​\te. 因为担心经济状况，而决定爱或者不爱算有条件的爱？\n\n  是，但是这是对于自己条件的check，而不是对于别人的条件的check。\n\n\n\n6/9  10:05\n\n​\t\t昨天收了电脑走在回宿舍的路上，我继续思考：我真的爱我的家人吗？想着想着我逐渐烦躁。\n\n​\t\t直接涌现出答案a的回答：我爱人肯定是有条件的。\n\n​\t\t因为对于我而言，所有有缺点的人我都不想爱。所有人都或多或少的优缺点，进而我不爱任何人，包括我自己。\n\n​\t\t这才是真实的我自己，不爱任何人。\n\n\n\n那么怎么感觉这两种属于不同的思考维度呢？\n\n-未完待续-\n\n\n\n## [自我设限：为了姿态好看，我可以连成功都放弃]([](6-8-vedio-review/2.mp4))\n\n**文案：**\n\n​\t\t越是面对不擅长的科目，就越喜欢在考前的复习时间，玩不到ddl前一晚就不会开始动工。这些看似普遍的行为背后的自动思维，其实都在慢慢的蚕食着人的勇气。上学的时候有老师会喜欢说这样的话，孩子很聪明不努力，说这句话的意图是鼓励孩子，但其实恰恰因为这样的话，不小心害掉了很多人，因为聪明可能是他得到的最大的，甚至可能是第一个夸奖。那么为了维持他自己都不知道具体有多大的聪明，就会越加恐惧失去他，认为努力会让人看到聪明的上限，所以如果不努力真的聪明就永远保有无限的可能性，但如果努力了，如果结果不尽人意，那么就连聪明的光环都没有了。另一方面，这种类型的思维在背后有一层潜台词，就是聪明和天赋是高贵的，而努力是廉价且丢人的，但这是一个非常错误的价值观。人类历史上每一位获得成功的高天赋的伟人都不是仅仅靠的天赋，而是他们的天赋和努力同样都异于常人，而那些天赋高却不努力的人都泯灭在历史长河中，没有人关心一个人的成就是占了多少比重的天赋和努力，也没法去判断。人们只会在意最终肉眼能看到的结果和成就，自我设限也叫**跛足策略**，是指人为的降低或回避不佳表现带来的负面影响，所采取的将失败原因外化的行动或选择，说白了就是想尽办法给自己制造点困难，目的就是为了最后说失败了不是我的错。这种思维的误区同时也是弊端，是从**一开始就默认自己不会做好**，于是开始给未来没有做好的自己提前布置一个体面的场子，但往往这最终都是一种自我诅咒。自我实现的预言是本可以成功的，你亲手把自己推向失败，本可以卓越的，你把自己推向侥幸及格，这是非常不划算的一笔账。所以我们应该意识到努力不是丢人的，努力和失败更不是丢人的，这是你在亲手为了自己的人生拼搏是非常值得自豪的事情。\n\n\n\n**我的感受：**\n\n​\t\tSadly，这种心理在我身上很早就出现。可能是因为有人说过我聪明，我喜欢这种夸耀和对于别人同学产生的心理阈值（从而产生对于学习的恐惧，是的，我是一个evil的人）。我自己虽然心里清楚，大部分归功于努力，却对外乐于接受别人的误解。后来慢慢的，我却逐渐害怕自己可能做不好的事，羞于承认自己做不到，开始在外部找原因。将失败的原因外包出去之后，我就不必承担责任，之后也不用努力了。类似于，自己给自己一个失败的预言，进而自己一步步的走向它，这的确是一种恶性循环。小到一场考试，大可以到人生是否可以幸福——是的，我一直怀疑我自己人生是否可以幸福。\n\n![1](6-8-vedio-review/2.png)\n\n\n\n## 改变自己从现在开始\n\n**文案：**\n\n It took me four years to learn what I'm about to teach you in the next minute and a half. **It is never too late to reinvent yourself.** You can decide that everything you have done up until this point in your life is irrelevant. You can choose today right now to like, start a completely new book, because **your past does not have to equal your future.** Unless you choose to live there for years. I thought the identity that I was creating in the early stages of my 20s was is going to be the identity I was gonna have for the rest of my life. It just doesn't have to be that way. I heard this quote from Tony Robbins and his just completely shaken my world. He says the biggest conflict that humans experience on a daily basis is the need to remain consistent with how they choose to identify themselves. And that need to remain consistent is usually what forces people to stay stuck in how they've always been. There's just so much peace and beauty and knowing that. Just because you did something one way for a long period of time, doesn't mean you have to keep doing it that way. You don't have to just like manage your day to day, manage your circumstances. You can actually be the creator of your own life. When that really clicked for me, I started to get excited about living again. The Sunday scary started to go away. I just got really excited about the future. And I want you to know that the same thing can happen for you. I'm telling you if you're feeling stuck in your life right now, do something differently, change something, make AA power move. You deserve a life on your terms. You deserve a life that you're freaking obsessed with. \n\n**译文：**\n\n在接下来的一分钟半里，我要教给你们我花了四年时间才学会的东西。**重塑自己永远不会太晚。**你可以决定，到现在为止，你所做的一切都是无关紧要的。你可以选择现在就喜欢，开始一本全新的书，因为你的过去不一定等于你的未来。除非你选择在那里生活很多年。我以为我在20多岁的早期阶段创造的身份将是我余生将拥有的身份。事情不一定非得是那样的。我听到托尼·罗宾斯的这句话，他的话完全震撼了我的世界。他说，人类每天经历的最大冲突是需要与他们选择的自我认同保持一致。这种保持一致的需要通常会迫使人们停留在他们一直以来的状态中。有太多的宁静和美丽，知道这一点。\n仅仅因为你用一种方式做了很长一段时间，并不意味着你必须一直这样做。你不必像管理你的日常生活一样，管理你的环境。你实际上可以成为你自己生活的创造者。当我真正意识到这一点时，我开始对再次生活感到兴奋。星期天的恐惧开始消失了。我只是对未来感到非常兴奋。我想让你知道，同样的事情也可能发生在你身上。我要告诉你的是，如果你现在觉得自己被困在生活中，做一些不同的事情，改变一些事情，让你的力量发生变化。你理应过上自行其是的生活。你值得拥有一种你疯狂痴迷的生活。\n\n## 不做韭菜，从【爱惜注意力】开始\n\n**文案：**\n\n​\t\t首先大家可能听说过注意力经济这个词，注意力经济它其实并不只是简单的我给你看广告，然后把东西卖给你，我火了以后带货直播不只是这样，即使你什么东西都不买，只要你盯着这个屏幕，他们就能够挣到钱，所以像是游戏电影红包都可以免费，你自己觉得我刷刷手机不花钱，但是对他们来说是很珍贵的注意力资源。所以在互联网这有这样一句话叫做免费的才是最贵。\n\n​\t\t有一位法国思想家西门大伟一他就说关注是最稀有最纯粹的一种慷慨，平时为什么我们那么慷慨的就把自己的注意力就给掉了？因为生活很无聊对吧？我想要找点乐子，然后现在有了这个东西以后，我们只要拿出来刷两下就无聊就消失了，很好用，但是它也有代价，大家应该都体验过刷了一天，然后啥事都没干成对吧？如果你有这样的体验的话，千万不要怪自己。去年我做了一个百万播放的视频叫做焦虑内耗，专注差不怪你是信息时代的多，怎么把这个锅甩给信息时代？大家刚刚有没有注意到你口袋里的手机振动了多少次？给你推送了多少信息？其实他们都是想要打断你此刻正在做的事情，把注意力你的注意力重新拉回到手机上去，对所有你看到的一切都是被精心设计过的。比如说像是朋友圈的红点，你有没有想过为什么不是黄点或者绿点还是红点？就是因为红色很扎眼，你就觉得我要把它点掉才开始，然后你一点然后5分钟就没了，然后他就获得了5分钟的用户使用时间，这样的注意力经济下去年中国人的平均手机使用时间已经达到了5个小时，而且这个还是跟你的爷爷奶奶平均过以后的，你的肯定更多。除了看手机的5个小时以外，其他时间我们其实也会想着刚刚看到的视频，刚刚看到的信息等等，总体上我们的注意力其实都处在一个挺糟糕的状态，不要说用在什么工作学习上可以非常的专注。我们现在其实即使连看一本书看一部电影对不对？都会觉得非常的困难。现在大家看电影都是那种什么三分钟看完，比如刚刚李校长讲的三分钟看完北京的阳光对吧？这个男人叫小川，这个男人叫大壮，他们去了挪威的海滩对吧？就是这样的视频。\n\n​\t\t所以你用到这样的注意力的方式，当你在碰到人生中真正重要的那些很艰难的很无聊的事情的时候，你其实就已经没有力气去推动了。如果你带着这样的思维方式走在重庆的街头，比如说晚上很繁华，其实还蛮可怕的，就你会觉得所有的这些声光电，然后所有的这些娱乐场所，烧烤摊的烟火气，其实它都在抓取你的注意力资源，怎么办？就没办法出家。我今天发了一个出家失败的视频，我既然还要在城市里面生活，有没有什么方式它是能够保存我们的注意资源，甚至再生让我们注意力变得更多的，有的不是你们想的，而是而是睡觉的。真的对于睡眠不太好的朋友，刚开始都会非常推荐说你先用到冥想来帮助你提高你的睡眠质量。我最早发在网上的比较火的冥想的视频，也是睡前用来帮助你入睡的，缓解失眠的这样一个视频。所以如果在座试过冥想，你可能冥想中会有这样的体验，真的觉得冥想中很瞌睡，这个完全不是什么问题，你只要需要把冥想的时间放到睡前，然后你觉得瞌睡了再去睡觉就可以。\n\n​\t\t所以我们通过比如说冥想来帮助你获得一个好的睡眠，第二天起床以后神完气足对，但是你不小心拿起了手机，好，然后下一次当你发现的时候半个小时就过去了，然后你想要开始做事情，发现心好像又已经累了，又没有专注力，所以光靠睡眠它本身并不能完全解决问题，这个时候就是冥想非常重要的极限。就像我们听到人力资源管理这个词，你可以把冥想理解为一种注意力资源的管理，对自己的我们现在就来简单的试一下，大家先不要闭眼睛，ok，此刻你觉得现在存在就是这个舞台在你的注意力之中，好，试试看，现在把注意力放在**你和舞台之间的距离**上，不要去思考它，直接去感受它，感受你和舞台之间的距离，这片充满了空气的空间，它一直存在的，只是我们刚刚没有关注它。好，我们的注意力继续往回拉，感觉一下自己的眼睛。如果你觉得眼睛有点疲劳，看了这么久，你可以选择闭上眼睛，感受闭上了以后他是否有颤抖，比较紧张，一直在发生的，我们没有注意到的，还有我们的**呼吸**，因为我们一直在自动的呼吸，所以我们才活着，感受一下空气的进出，感受一下肺部腹部的起伏，感受一下这种注意力保持在内在的状态，在下一次呼吸之后可以慢慢睁开眼睛，是不是感觉不太一样？愿不愿意每天少玩两分钟的手机用来做一下分享？谢谢。简单的和大家讲一下冥想的原理跟注意力与资源有关的话，在这里讲到两个点，第一个是你的注意力储存的能力，平时我们的注意都是往外耗散掉的，可能是你主动选择的事物，也可能是被外界当时吸引的。但是就像在刚刚的练习中，我们学习把注意力从前面拉回来，保存在身体内，这样当我们看到真正重要的事情的时候，是神玩气质的可以去做它。第二种能力是选择的能力，就像刚刚其实我们感觉的东西它其实一直存在对不对？但是如果我没有做集市，你可能一直注意到的只是这个舞台，但是现在我们可以选择把注意力放在不同的维度上，这就是一种选择的能力。\n\n​\t\t而日常生活中，当我们碰到一个诱惑你的对象或者某一个思绪的杂念的时候，冥想不是说你要完全把它去除，而是我有选择的人，我可以选择把注意力拉回来，用在我觉得值得的地方。所有刚刚讲的冥想的原理，它的这些能力其实背后都是有非常多的科学研究证实的，这也是为什么冥想这些年会在西方开始火，然后火到东方来，比如说跟注意力资源的控制有关的部分，其实科学证实随着冥想练习它都是会获得物理性的改变的。举个例子，比如说我们大脑的前额叶的位置，就是在你的额头光光的地方，它是跟自控自律有关，特别跟注意力相关。当你在长时间练习冥想以后，你的前额叶除了它的活跃度提高以外，甚至它的厚度都会变后**很惊人的就像你在健身一样，练久了以后肌肉真的就变强壮**了，你就可以推起来了。所以影响练习久了以后，你的注意力的控制真的会变得非常的强。Ok，假如你觉得博主说的挺有道理的，然后我想试试冥想，那怎么去学习冥想？我虽然在前面抨击了一些现代科技，但我本身其实是不反对科技的，比如说我自己也把冥想上传到网上，因为我觉得用手机来播放冥想音频特别的方便，然后它真的可以节省很多的金钱时间以及跨越时空。像是我是第一次来重庆，然后这次就看到很多热情的怪兽部落的朋友，他说李海老师你的冥想音频太好了，我每天都是靠它催眠的，就是我觉得虽然有点挫败，但是总是一件好事情。然后我其实最早把理想上传到旅游b站的时候，只是因为它能放比较长的视频，我当时其实完全不指望说年轻人z时代可以变联想对吧？不是已经要被游戏毁掉了，大家的注意力都很差，根本坐不住，结果是让我非常震惊的。其实年轻人很清楚，自己的大脑在现在的信息朦胧面前已经不怎么够用了，所以我真的很惊讶，竟然有那么多的人在练习冥想，所以他们真的普遍的反映说注意力得到了改善，比如说效率精力的提高，睡眠和情绪的稳定等等。后来 B站发现了以后，他就觉得影响可以培养一下。然后我最早很有意思的，我最早是把冥想视频传到知识区的，因为我觉得这个东西比较对吧？知识分子里面就是罗翔老师一个群，后来他们把我的视频移到了另外一个群，你们猜是什么群？什么鬼畜区肯定不是对生活区音乐区，他们竟然传把冥想移到了运动区，我当时就觉得冥想这么静态的一个东西对吧？然后怎么放到运动去，我当时觉得有点奇怪，后来我觉得还挺不错的。我觉得冥想真的有希望变成一个像运动一样，在日常生活中平常的很普遍的一个事情，大家会觉得说他很奇怪，我真的觉得他有点像20年前的健身，就像20多年前那个时候健身这个词才刚刚出来，对吧？大家不会觉得说我要专门花钱去健身房锻炼，但是就像之前讲到的各种像GDP这些，我们人类的发展肯定是慢慢的从体力活动体力劳动过渡到脑力劳动，所以之前我们就发现我还是需要一些活动来保持我的身体健康的，就像大家知道现在所以就觉得想要动一动对吧？所以我们有了健身，他曾经其实也是非常小众的，从像是杂技对吧？武术那样的小众的运动里面提取了一些简单的元素，然后把它做成了健身。现在大家都已经普遍接受了，我觉得像运动一样影响其实也很有这个可能。就像大家知道现在可能就觉得已经有点累了，是不是有点涣散，听不懂，或者回到家躺上床以后，大脑正常的运动，出于这种原因，所以我们从比如说山里的影视，从苦修者苦行僧那边，我们找来了一些很基本的但是效果不错的那些练习，构建成了现代的冥想，就是希望可以帮助大家保持心理的健康。最后回到这句话，关注是最稀有最纯粹的一种慷慨。那么首先非常感谢大家给了我15分钟这样的慷慨作为回报，我在网上的练习资源大部分是免费的，我是希望可以帮助大家，不是不要变成一个0和游戏对吧？你自己的注意力还是重新可以再升起来，是可以变到一个饱满的状态，从而帮助你可以做的这个事情。可以看进书，回到家以后你可以听得进，你家人跟你说的话，其实现在已经变成了很奢侈的一件事情，以及我觉得注意力其实不应该被异化成一种资源，就像这里讲的关注是最稀有最纯粹的一种慷慨。很多人会觉得一个人一个冥想的人坐在那边好像是一个很被动很无力的状态，但其实不是的。当你在冥想的时候，就像这句话说的，把顺序倒转一下，你其实是很慷慨的把关注给到了自己，就像灯光师这个时候可以把敞灯前面关掉以后，只剩下中间的聚光灯了。我们平时看惯了这种分散的灯光，你其实很少看到这样的灯光，但是当你能够聚焦的时候，你会获得一种最罕见最纯粹的一种力量，而这种力量是可以贯穿一切的。我头上丸子头等我以后练到大师的时候，光聚的更小，然后有一天烧掉了，然后就没有练出来了。所以冥想人其实就是在获得这样的一种专注的力量。Ok，今天分享就到这边，谢谢大家。\n\n **提取信息：**\n\n- 冥想是值得日常执行的hobby\n- 比较好的时间是睡前\n- 睡前睡后远离手机\n\n## 节食减肥后忍不住暴食，反弹了20斤\n\n**文案：**\t\t\n\n​\t\t为什么利用结石挨饿去快速减重是爆肥的起点，对，你看到秤上的数字减少了，但你没看到的是在长期帮你消耗热量的肌肉也减少了，新陈代谢减慢了，基础消耗降低了，而当你恢复曾经的正常饮食后，新陈代谢和基础消耗却没有跟着恢复回来，那么多余的热量就会囤积成脂肪，没错，你减掉的是**水分和肌肉**，而反弹回来的却是**纯脂肪**，由于结石造成的各类必需营养素的缺失，会带来长期不可逆的器官损害，大脑神经功能的衰退更会直接引起消极负面情绪，你会莫名的感受到烦躁沮丧，甚至焦虑抑郁，而消极抑郁的情绪则会直接影响学习的效率，工作的进度、认知的提升，人际关系的处理，最终导致你生命质量和幸福程度严重下降，从而引起压力。荷尔蒙超量分泌在焦虑抑郁狂躁不安的情绪中，你会控制不住的开始暴饮暴食，饥饿的身体长期缺乏必需营养素和必需蛋白质，大脑就会不断接收到威胁的信号，所以补救的第一步就是补补充营养和大量蛋白质，我们能从进化论的角度来看，节食减肥这件事就不难解释它为何会引发暴饮暴食了，在自私的基因这本书里提到过，人的一切行为都是由基因操控着的，而基因的本能是延续而延续，则需要生物个体体能的增强，生存能力的提升，而节食减肥减少热量摄入则是让个体变弱，降低生存能力，这时基因就会感受到威胁，从而疯狂的向大脑下达储存更多热量的命令，让你无法抑制的暴食。所以每当学员问我到底什么减肥方法才是最好的，让你不觉得是一份痛苦的工作，并可以毫不费力的坚持下去的减肥方法。就是最好的，一旦当你开始顺从人性，不再去对抗基因的本能，从小事开始去培养健康生活的惯性，维持一个健康理想的身材，会变成一件很轻松的事情。比如在饮食上记住赶走肥肉的不是饥饿感，而是饱腹感，把膳食中精致的高糖高油脂食物逐步替换成粗制的加工程度低的胃排空速度慢的食物热效应高的、热量密度小的，膳食纤维高的，以更低的热量让自己获得同样的饱腹感和满足感。\n\n​\t\t在生活上**遵循小步子法则**，欲速则不达，每天前进一点点不确定难以坚持的目标，给生活增加压力，比如每天每天运动10分钟，可以从多爬几层楼梯开始，从不坐公交骑车回家开始，从看电视的时候打开欧阳春小沙露腰边看边做开始，等到身体的神经和肌肉产生了适应，有了惯性，把运动视为刷牙洗脸一样的习惯，再慢慢加时长和强度，同时了解运动初期的体重上涨是因为受体重的增加，糖原储存能力的增加，血容量的增加，而不在焦虑和沮丧，并为自己长期维持健康身材的能力逐渐变强而感到骄傲，不去期待结果，只去做你认为该做的事儿，你会发现你曾经最在意的秤上的数字，在体脂率变得更低时，心跳变得更强有力，时心肺功能大幅提升时，骨骼变得更加强韧时，肌肉变得更加紧实，有形时关节变得灵活又稳定时皮肤变得光滑透亮，状态变得明媚松弛，气场变得自信，强大时，体重变成你最不在意的事儿。\n\n**我的感受：**\n\n​\t\t经历过健身、节食和跳操之后，我发觉健身是我最喜欢的保持健康的方式。是的，我说的是保持健康，而不是保持怎样的身材。我理想中的我技术精炼，知识丰富，身体健康的人。我把“减肥”这个词从我的字典中删除了。又扔掉一个无用的东西，爽~\n\n \n\n## 裹 以 蜜 糖\n\n<img src=\"6-8-vedio-review/image-20230609163854316.png\" alt=\"image-20230609163854316\" style=\"zoom: 50%;\" />\n\n**现象：**\n\n​\t\t全人类正在变得浅薄，空洞，敲两下的话，会有梆梆两声。而女人尤其如此。\n\n​\t\t她们就像被降了智，惊慌，自我设限，讨好。\n\n<img src=\"6-8-vedio-review/image-20230609164337521.png\" alt=\"image-20230609164337521\" style=\"zoom: 33%;\" /><img src=\"6-8-vedio-review/image-20230609164419039.png\" alt=\"image-20230609164419039\" style=\"zoom: 33%;\" />\n\n<img src=\"6-8-vedio-review/image-20230609164455947.png\" alt=\"image-20230609164455947\" style=\"zoom: 33%;\" /><img src=\"6-8-vedio-review/image-20230609164631310.png\" alt=\"image-20230609164631310\" style=\"zoom: 33%;\" />\n\n**我的感受：**\n\n​\t\t我应当保持警醒：分辨出什么是我真正需要东西？什么是我真正想要的东西？\n\n​\t\t什么是我感受到真正的快乐？自由让我快乐，经济上的自由、社会上的自由令我快乐；和家人安定的生活令我感到快乐；坚持了很终于得到想要的录取通知书令我快乐；在自己的领域有所成就使我快乐；在人类历史上留下我的名字使我快乐。\n\n​\t\t为了一时的错觉的快乐，获得睡眠不安的夜晚是不值得的。\n\n\n\n​\t\n","tags":["review"],"categories":["video-review"]},{"title":"Diary 6/4","url":"/2023/06/04/6-4-diary/","content":"\n[toc]\n\n# 今日\n\n- 10点左右起床，复习到14：00去开会，哇，去了感觉一屋子的loser、shit，而我就是其中的一员，ewwwww。\n\n- 然后15：50去考大物，啥也不会，但是必须想办法及格。\n- 本科学费没有着落\n- 工作也没有着落\n- 生活费也基本不够花\n\nmy life so suck\n\n","tags":["diary"],"categories":["diary"]},{"title":"Diary 6/2","url":"/2023/06/02/6-2-diary/","content":"\n[toc]\n\n# 形容我现在的状态：\n\n浑浑噩噩\n\n\n\n# 关于化妆：\n\n大写的no！！\n\n1. 不舒服\n2. 花费高\n3. 可能损伤皮肤\n\n\n\n# 关于护肤\n\n考虑酌情减少\n\n1. 夏天不怎么舒服\n2. 冬天按需要护肤\n3. 水乳后来发现并不是必要的\n4. 精简护肤：珂润面霜、适乐肤乳、943防晒霜、凡士林唇膏\n\n\n\n# 关于穿衣服\n\n不想穿让我感觉不舒服的衣服\n\n（写这些的时候，正在穿着jk，好不喜欢这种感觉：容易走光，款式幼稚，颜色花哨）\n\n之后挑选衣服consider的点的排序：\n\n1. 舒适！尤其是鞋子，内衣、内裤、袜子\n2. 给人威慑感，力量感的衣服\n3. 材质有质感，显贵气\n4. 颜色黑白灰，好搭配\n\n# 关于身材管理\n\n1. 最重要的是健康\n2. 保持健身\n3. 学习拳击，攻击人的那种\n4. 为了开心可以学习舞蹈\n\n# 其他\n\n最近吃的不委屈自己，想吃什么吃什么的感觉太好了吧！！\n\n感受得到了美食的快乐！！\n\n不会再节食了\n","tags":["diary"],"categories":["diary"]},{"title":"June Schedule","url":"/2023/06/01/6-1-june-schedule/","content":"\n[toc]\n\n# todo\n\n- [ ] 整理毕业资料\n- [ ] 大物考试\n- [ ] 材料之美论文\n- [ ] 高数考试\n- [ ] 借钱交学费\n- [ ] 求职华为\n\n\n\n# life road\n\n\n\n\n# schedule\n\n|      |                mon                 | tue  | wed  | thu  |      | sat  |                sun                |\n| :--: | :--------------------------------: | :--: | :--: | :--: | :--: | :--: | :-------------------------------: |\n|  15  |                                    |      |      |  1   |  2   |  3   | <span style=\"color:red;\">4</span> |\n|  16  |                 5                  |  6   |  7   |  8   |  9   |  10  |                11                 |\n|  17  |                 12                 |  13  |  14  |  15  |  16  |  17  |                18                 |\n|  18  |                 19                 |  19  |  20  |  21  |  22  |  23  |                24                 |\n|  19  | <span style=\"color:red;\">26</span> |  27  |  28  |  29  |  30  |      |                                   |\n","tags":["preview"],"categories":["monthly-preview"]},{"title":"Graduate Paper","url":"/2023/06/01/6-1-my-paper/","content":"\n[toc]\n\n![img](6-1-my-paper/wps1.jpg) \n\n# 毕业设计说明书\n\n \n\n \n\n|                            作  者                            |             山杜哈西·土鲁四拜克              |\n| :----------------------------------------------------------: | :------------------------------------------: |\n|                            学 号                             |                 919106840208                 |\n|                             学院                             |             计算机科学与工程学院             |\n|                          专业(方向)                          |                智能科学与技术                |\n|                            题  目                            | 基于图神经网络和注意力机制的社交推荐算法研究 |\n| class Solution{​ public:    vector<int> sortedSquares(vector<int>& nums){        int len = nums.size();        vector<int>ans(len);​        int l=0;        int r=len-1;        int idx=len-1;​        while(idx>=0){            if(nums[l]*nums[l]>nums[r]*nums[r]){                ans[idx]=nums[l]*nums[l];                l++;            }else{                ans[idx]=nums[r]*nums[r];                r--;            }            idx--;        }​        return ans;    }​};​c++ |                孟顺梅 副教授                 |\n|                            评阅者                            |                杜鹏桢 副教授                 |\n\n \n\n \n\n2023  年  5  月\n\n \n\n \n\n\n\n \n\n \n\n# 声\t明\n\n \n\n我声明，本毕业设计说明书及其研究工作和所取得的成果是本人在导师的指导下独立完成的。研究过程中利用的所有资料均已在参考文献中列出，其他人员或机构对本毕业设计工作做出的贡献也已在致谢部分说明。\n\n本毕业设计说明书不涉及任何秘密，南京理工大学有权保存其电子和纸质文档，可以借阅或网上公布其部分或全部内容，可以向有关部门或机构送交并授权保存、借阅或网上公布其部分或全部内容。\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n\n# 中文摘要\n\n 摘要：\n\n社交网络的日益普及，促进了社交推荐领域的迅猛发展。随着数据规模的不断增大，社交推荐问题变得越来越复杂和具有挑战性。为了解决传统推荐算法存在的问题，本文提出了一种基于图神经网络和注意力机制的社交推荐算法。该算法通过将用户行为和社交网络信息融合到一个图中，然后使用图神经网络对图进行学习，获取每个用户和项目的嵌入向量。同时，引入注意力机制，以更好地捕捉用户在社交网络中的交互和兴趣相关性。最后，利用学习到的嵌入向量和交互行为信息进行推荐。实验结果表明，该算法 在三个真实数据集上的表现比现有的深度学习推荐系统都有较大提升，证明了该算法在社交推荐中的有效性。   \n\n关键词：\n\n图神经网络 社交推荐算法 注意力机制 用户兴趣建模 \n\n \n\n\n\n# 毕业设计说明书外文摘要\n\n **Title:**\n\nResearch on Social Recommendation Algorithm Based on Graph Neural Network and Attention Mechanism  \n\n**Abstract:**\n\nThe increasing popularity of social networks has promoted the rapid development of the field of social recommendation. With the continuous increase of data scale, the social recommendation problem becomes more and more complex and challenging. In order to solve the problems existing in traditional recommendation algorithms, this paper proposes a social recommendation algorithm based on graph neural network and attention mechanism. The algorithm combines user behavior and social network information into a graph, and then uses a graph neural network to learn the graph to obtain the embedding vector of each user and item. At the same time, an attention mechanism is introduced to better capture user interactions and interest correlations in social networks. Finally, recommendations are made using the learned embedding vectors and interaction behavior information. The experimental results show that the performance of the algorithm on two real data sets is greatly improved compared with the traditional matrix factorization and deep learning methods, which proves the effectiveness of the algorithm in social recommendation. \n\n **Keywords：**\n\ngraph neural network, social recommendation algorithm, attention mechanism, user interest modeling\n\n \n\n\n\n \n\n\n\n# 1 引言\n\n近年来，社交推荐算法作为社交媒体中的重要应用之一，受到越来越多的关注。然而，传统的协同过滤和内容推荐算法在社交推荐中存在着数据稀缺、冷启动问题、长尾效应等诸多局限和挑战。因此，如何利用图神经网络和注意力机制来解决这些问题成为当前社交推荐算法研究的热点。基于图神经网络和注意力机制的社交推荐算法可以充分提取社交网络的结构特征和情感语义信息，从而更准确地推荐用户感兴趣的内容，提高质量和效率。本文旨在对此类算法进行综述和探索，以期为社交推荐算法的开发和应用提供一些参考。\n\n## 1.1 研究背景及其意义\n\n随着网络技术的飞速发展和社交网络的普及，社交网络已经成为人们日常生活中不可缺少的一部分。社交网络的用户数量正在快速增长，积累了庞大的用户行为数据，成为进行社交推荐的宝贵资源。所谓社交推荐，是指在社交网络中，利用用户的社交关系、兴趣、行为等信息，向用户介绍个性化、多样化的内容和服务。在社交推荐中，传统的推荐算法，如基于内容的推荐、协作过滤、隐语义模型等，从单一角度对用户进行建模推荐只能进行修改，很难从多个角度理解用户的兴趣和行为，导致推荐的准确度和多样性不足。\n\n近年来，深度学习技术的兴起为解决传统推荐算法的局限性提供了有效的解决方案。深度学习是一种具有多层神经网络的机器学习方法，其可以自动地从数据中学习多层抽象特征，表达数据的内在结构并对新的数据进行推理和预测。其中，基于图神经网络(Graph Neural Network,GNN)和注意力机制(Attention Mechanism)的社交推荐算法，则成为了当前研究的热点之一。\n\nxxxxxxxxxx class Solution {    public int[] searchRange(int[] nums, int target) {      int [] arr={-1,-1};            int left = binarySearch(nums, target, true);            int right = binarySearch(nums, target, false)-1;​            if(left<=right && right<nums.length && nums[left] == target && nums[right] == target){                arr[0]=left;                arr[1]=right;            }             return arr;​    }        public int binarySearch(int[] nums, int target, boolean isLeft){            int low = 0, high = nums.length-1, mid, ans = nums.length;            while(low<=high){                mid=(low+high)>>1;                if(target < nums[mid]|| isLeft && target <= nums[mid]){                    ans = mid;                    high = mid - 1;                }else{                    low = mid + 1;                }                            }            return ans;        }}java\n\n除了基于图神经网络的算法，注意力机制也被广泛应用于社交推荐算法中。注意力机制是一种重要的机器学习技术，通过给不同节点或边赋予不同的权重来调节模型的关注度，从而使模型更关注于对当前任务最有用的信息。在社交推荐中引入注意力机制，可以使得模型更加关注用户和物品之间最为重要的关系和信息，从而提高推荐的精度和效率。事实上，注意力机制已经成为当前最先进的深度学习技术之一，其应用及变种层出不穷。通过将注意力机制应用于社交推荐中，不仅可以提高推荐效果，还可以在复杂的网络结构中发现隐藏的信息和规律，这有助于发现社交网络中影响用户兴趣和行为的关键因素，为执行更加有效的社交推荐提供支持。\n\n因此，研究基于图神经网络和注意力机制的社交推荐算法不仅有助于解决社交推荐领域的问题，还可以推动其他领域的发展。这种研究具有重要的理论和实际意义。通过研究基于图神经网络和注意力机制的社交推荐算法，可以提高推荐服务的精度和多样性，更好地满足用户需求。同时，该研究也有助于推动深度学习技术的发展和创新，为未来的研究提供有价值的参考。\n\n## 1.2 国内外研究现状\n\n基于图神经网络和注意力机制的社交推荐算法是近年来热门的研究方向之一。国内外许多研究者针对这一算法进行了深入的探讨和实验研究。本小节将对其国内外的研究现状进行详细介绍。\n\n### 1.2.1 基于图神经网络的推荐系统研究现状\n\n近年来，图形表示作为一种强大的建模方法得到了广泛的研究，并应用在各个领域：社交媒体中的友谊分析[ 1]、生物学中的蛋白质相互作用建模 [2]、社会学中构建推理知识图等 [3]。\n\n在推荐系统领域中，基于图的数据建模可以通过扩展相似性来覆盖实体之间的关系，从而帮助克服稀疏性等问题[ 4]。图建模的推荐系统中定义相似性有多种方法，包括元路径分析 [5]、 模拟随机游走[ 6]和网页排序算法 [7]。然而，这些方法的主要问题是，不清楚哪条路径可以反应图中实体之间的相似性[ 4]。随着学者们进一步研究发现，该相似性的计算可以被用户和 项目节点的嵌入所取代。而图神经网络就是一种通用的方法，用于提取图中节点的嵌入。 \n\n图神经网络（GNN）是一种从卷积神经网络（Convolutional  Neural Net-works，CNN[38]）启发而来的新型神经网络，它可以从节点信息和图的拓扑结构中学习[ 8]，并且有助于合并用户和项目之间的多跳连接信息 。图神经网络通常使用聚合模块和更新模块来嵌入节点。图神经网络可以决定将多少信息从一个节点传播到它的邻居节点，以及如何聚合从邻居节点接收到的所有信息，同时考虑节点之间相应边的权重[ 9]。其中聚合模块和更新模块可以使用不同的方法：在聚合模块中，Ying等人[ 10]使用最大池化和平 均池化进行聚合邻居节点信息，而Hamilton等人[ 2]使用了LSTM聚合器。在更新模块中，He 等人[ 11]将聚合信息作为目标节点的新表示；Hamilton等人 [2]将聚合信息与目标节点的原始信 息进行聚合获得最终嵌入。Monti等人[ 12]在提取嵌入信息的过程中，将附加信息与用户反馈结合在一起，并且修改了ChebNet[ 13]中的卷积运算，从用户图和项目图中分别提取用户和项目嵌入，通过使用循环神经网络（Recurrent Neural Network，RNN）的扩散过程逐步改善提 取的嵌入，用来正确预测未知反馈。Berg 等人[ 14]提出的GC-MC模型是第一个在推荐系统中使用图自动编码器 （Graph Auto Encoder，GAE）的模型，与专注于节点嵌入的模型不同， 该模型直接通过单个非迭代图神经网络提供预测，并且使用了图卷积编码器和解码器来预测用户项目节点之间边的权重。Fan等人[ 15]提出的GraphRec模型中，用户信息由社交网络中的邻居用户和该用户本身两部分组成，使用平均池化来聚合邻居的信息。利用多层感知机对未知反馈进行预测，该多层感知机基于邻居用户获得目标项目表示，基于其用户和邻居获得目标用户表示。Wu等人[16]是第一个提出基于会话的图神经网络推荐系统。 \n\n图卷积神经网络（Graph Convolutional Network，GCN）是最常用的图神经网络方法， 它根据节点之间的权重组合节点的特征向量，对节点进行编码。用双线性解码器获取用户和 项目的编码向量，并重建用户反馈。主要分为两大类：基于谱（spectral-based）和基于空间 （spatial-based）。谱图卷积神经网络表示基于拉普拉斯矩阵的图，计算拉普拉斯矩阵需要大 量的计算时间。并且也不可能将经过训练的模型从特定结构转移到另一个图形。另一方面， 空间图卷积神经网络在处理不同大小的邻居节点和节点权重矩阵方面也存在一些挑战。\n\nPinSage模型是Ying等人[ 10]通过图卷积神经网络开发的，也是图卷积神经网络模型第一次成功应用于商业推荐系统。PinSage没有使用k-hop算法，而是基于随机游走算法对相似的无向 邻居节点进行采样，然后将生成的嵌入与节点的初始嵌入连接起来，并通过另一个密集层来 获得最终表示。Zhang等人[ 16]提出了IG-MC模型，该模型中对于每个用户的交互项目，提取 一个包含用户和项目的封闭子图，与PinSage不同，后者通过提取子图的嵌入，对该子图节 点之间的相关性进行建模，然后聚集节点来表示子图。Sun等人[ 17]提出的Multi-GCCF模型 中，把图卷积神经网络应用于用户图的邻域信息中，将节点信息与一阶邻域用户节点信息合 并嵌入。在NGCF模型[ 18]中，用户和项目的高阶连通性被考虑在用户项目二分图中，并且在聚合了高阶邻居信息的同时，该模型还通过聚合不同阶的嵌入来表示节点的最终嵌入。为了收集用户兴趣点的偏好数据，Zhong等人[ 19]在图卷积神经网络上融合了多头注意机制。\n\nHekmatfar 等人[ 20]提出的PGRec模型可以预测用户和偏好节点之间的连接权重。该图采用了单层谱图卷积神经网络，可以基于目标节点和每个邻域之间的路径数量，计算每个间接邻域的权重，从而确定目标节点中每个邻域的影响量。Duran等人[ 21]提出的基于会话推荐的增强 图神经网络模型可以充分利用上下文信息，并在用户上下文项目图上应用图卷积神经网络。 其中用户节点是基于该节点交互项目的传播信息，以及用户和项目之间交互的上下文来表示的。Song等人[ 22]提出的GACSE模型考虑了静态嵌入（即所有节点的归一化平均值）和动态嵌入，并且通过图注意力机制计算邻居的加权平均值。他们还提出将贝叶斯个性化排序和节 点相似性相结合，作为一个面向排序的损失函数。 \n\n图注意力网络（Graph Attention Networks，GAT）是一种空间图卷积神经网络方法，在 图神经网络的传播步骤中采用注意力机制[ 23]。它根据每个邻居节点与目标节点的相似性计算 每个邻居节点的注意力系数，该相似性是从其相应的特征向量中推导出来的[ 24]。注意力机制以多种方式应用于图注意力网络，Chen等人[ 25]为不同特性的邻居节点分配不同的注意力系数。在异构图中，注意力机制主要用于为每种类型的节点[ 26]和边 [27]或不同的元路径 [5]分配 不同的系数。在频谱图神经网络中，图注意力网络从低频（与小特征值相关的特征向量）到 高频（与大特征值相关的特征向量）提取不同频率分量的重要性[ 28]。为了评估每个用户的社会影响，Wu等人[ 29]首次在推荐系统上应用了图注意力网络。该模型考虑了社交网络中每个 邻居的排他性权重。\n\n### 1.2.2 融合注意力机制的推荐系统研究现状\n\n随着信息技术的快速发展和网络交流的普及，线上交流和沟通已经成为人们日常生活不可或缺的一部分，这也为推荐系统带来了新的发展机遇。 研究者们开始探索使用注意力机制来优化[ 30]推荐系统并取得了显著的进展，通过大量实验[ 31]验证，证明了注意力机制可以有效提高推荐系统的准确性。\n\n根据Pei等人的观点[ 32]，现有的推荐系统中通常将用户项目的全部历史信息视为同等相关，但这种处理方式并不适用于现实场景。 柴玉梅等人[33]发现，在评论文本驱动的推荐系统中，很多方法只考虑用户对项目的评分和评论，却没能充分挖掘评论文本中蕴含的有价值的信息。为此，研究者们提出了一种双重注意力机制来增强对文本信息的关注并获取更多有意义的信息，从而改善推荐系统的性能。Chen等人[34]在因子分解机器中应用神经注意力机制网络来学习用户与项目之间的交互特征，并将注意力网络组件进行聚合。将注意力机制和项目结合后，研究者们成功地获取了用户的最终表示，并将其用于推荐任务。研究者Guo等人[35]提出了一种神经注意力推荐模型，利用用户的隐式反馈行为，将用户转化为向量形式，然后从用户的项目历史中提取时序信息。\n\n## 1.3 总体技术方案及其社会影响\n\n本课题要解决的问题是基于图神经网络和注意力机制的社交推荐算法研究，解决思路步骤是研究并掌握现有技术、构建模型、选取合适数据集、训练模型、比较结果验证其有效性。研究并掌握多种融合注意力机制和图神经网络的社交推荐模型（如 DiffNet++[40]，ASR，LightGCN[11] 等），研究图经网络技术，与现有推荐模型相结合。通过对用户的历史交互行为分别分析和构建用户偏好与项目相关的模型，融合注意力机制，利用获取的用户及项目特征进行推荐，基于图神经网络和注意力机制的社交推荐算法。最后选择了LastFM、Yelp 和Douban-book数据集，对提出算法进行实现，并与现有推荐技术（DiffNet[39]、DiffNet++[40]和LightGCN[11] ）进行比较，通过 Recall 和NDCG两个评价指标来论证模型的有效性。\n\n该算法的社会影响主要有两个方面。首先，推荐系统对于提高信息的个性化呈现、提高用户体验、提高品牌商家曝光度和提高销售业绩等方面起着重要作用。其次，该算法使用到了图神经网络和注意力机制等前沿技术，为相关领域提供了研究参考。\n\n## 1.4 技术方案的经济因素分析\n\n本课题充分考虑了技术方案的经济可行性，选取了合适的数据集便于普通的个人电脑GPU训练模型，无需额外购买设备，可行性高。\n\n## 1.5 论文章节安排\n\n本文主要分为五个章节，详细安排如下所示：\n\n第一章节，绪论部分，简要介绍了课题的研究背景及其意义，详细描述了国内外研究现状和重点，并介绍了本文主要工作。\n\n第二章节，理论部分，介绍了推荐系统、图神经网络、注意力机制等相关的基础理论知识。\n\n第三章节，模型部分，提出了基于图神经网络和注意力机制的社交推荐算法研究，模型结合图神经网络和注意力机制基于用户之间社交关系进行推荐。\n\n第四章节，实验部分，将模型应用于多个真实数据集，并探究了模型各参数对推荐性能的影响程度，与当下几个主流推荐算法进行对比分析，验证了模型的有效性。\n\n第五章节，总结部分，梳理了本课题成果和改进之处，并对下一阶段学术研究工作进行展望。\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n# 2  相关技术理论\n\n本章将详细阐明推荐系统技术、图神经网络模型以及注意力机制相关的基础理论知识。\n\n## 2.1 推荐算法\n\n推荐算法是计算机科学和人工智能领域的一个重要分支，也是互联网和移动互联网应用中必不可少的一部分。推荐算法通过对用户历史行为、个人偏好、社交关系等信息的分析，为用户推荐个性化的内容，提高了用户体验和满意度，并对商业价值的实现起到了重要的促进作用。\n\n推荐算法的关键在于利用机器学习、数据挖掘等方法来分析用户的行为和兴趣，以便向用户推荐最符合他们需求的内容。一般而言，推荐算法主要有三种基本方法，即基于内容的推荐、协同过滤推荐和基于规则的推荐。\n\n基于内容的推荐方法是根据物品的属性信息（如电影的演员、导演等）计算物品之间的相似度，从而为用户推荐与其已经喜欢的物品相似的物品。该方法的优点是可以推荐新物品，但是缺点是很难捕捉用户的兴趣演变趋势。\n\n协同过滤推荐方法通过用户行为数据（如用户购买、评价、浏览等记录）寻找用户和物品之间的关联关系，从而推荐用户可能感兴趣的物品。该方法的优点是可以适应用户兴趣的演变和变化，但是缺点是很难处理稀疏数据。\n\n基于规则的推荐方法则是根据预先设定的规则筛选出用户可能感兴趣的物品。该方法的优点是不需要考虑用户行为数据，但是缺点是需要手动设定规则，且规则使用效果容易受到规则设置的限制。\n\n近年来，随着互联网应用的不断发展和数据量的增加，人工智能技术的进步也带来了推荐算法的新变革，如基于深度学习的推荐算法、基于知识图谱的推荐算法等。\n\n基于深度学习的推荐算法通过深度神经网络学习用户和物品之间的交互特征，进而预测用户可能感兴趣的物品，并实现个性化推荐。该方法可以从多个角度和维度对用户进行建模和理解，进一步提高了个性化推荐的准确性和独特性。\n\n基于知识图谱的推荐算法则是通过将用户行为数据映射到结构化的图形知识库上，进而针对用户和物品进行推荐。该方法可以使用多种关系和约束来描述用户和物品之间的关系，进一步提高了推荐的准确性。\n\n### 2.1.1  基于内容的推荐算法 \n\n​    基于内容的推荐算法是一种常用推荐算法，它可以根据物品之间的相似性计算得分，然后推荐与用户过去喜好相似的物品（如图2-1）。该算法主要利用物品的内容信息来计算相似度，并且在推荐系统的音乐、电影、新闻等领域得到广泛应用。\n\n![img](6-1-my-paper/wps2.jpg) \n\n图 2-1 基于内容的推荐算法\n\n基于内容的推荐算法实现流程包括以下几个步骤：首先，从物品的描述信息中抽取出所需的特征，如标签、分类、关键词等，并将它们转化为机器可理解的表示方式，例如向量或矩阵。然后，对于这些表示方式，进行常规的相似度计算方法，例如余弦相似度或欧几里德距离等，来计算物品之间的相似度。最后，根据计算出的物品相似度进行相似物品推荐，选择与目标物品相似度最高的物品，将它作为推荐结果返回给用户。\n\n### 2.1.2 协同过滤算法\n\n协同过滤算法是一种广泛应用的推荐算法，它综合了基于内容的推荐算法的优点，并利用用户历史行为数据，预测目标用户的兴趣偏好，从而实现了更加准确的推荐。协同过滤算法主要有两种模型：基于用户的协同过滤和基于物品的协同过滤。基于用户的协同过滤算法通过分析用户之间的相似性，将目标用户的兴趣偏好与相似用户的偏好进行比较，来进行推荐。而基于物品的协同过滤算法则通过分析物品之间的相似性，将目标用户曾经喜欢的物品与相似的物品进行比较，从而进行推荐。\n\n(1) 基于用户的协同过滤算法\n\n基于用户的协同过滤算法通过分析多个用户的历史行为数据，找到与目标用户具有相似兴趣偏好的其他用户，并向目标用户推荐这些用户喜欢的物品。具体的实现流程如下：首先，收集目标用户的历史行为数据，例如浏览、点击、购买等信息。接着，通过相似度计算方法，例如余弦相似度、欧几里德距离等，计算目标用户和其他用户之间的相似度，并选择与目标用户相似度最高的K个用户。最后，从这K个用户中挑选出他们喜欢的物品，并将这些物品推荐给目标用户（如图2-2）。\n\n![img](6-1-my-paper/wps3.jpg) \n\n图 2-2  基于用户的协同过滤推荐系统\n\n(2) 基于物品的协同过滤算法\n\n基于物品的协同过滤算法利用物品之间的相似性关系，找到与目标物品相似的其他物品，并向用户推荐这些相似的物品。具体的实现流程如下：首先，收集目标用户的历史行为数据，例如浏览、点击、购买等信息。然后，分析历史行为数据，找出目标用户对哪些物品感兴趣，以及哪些物品与目标物品相似。接着，基于物品相似度，从相似的物品中选取一定数量的物品作为推荐结果进行推荐给用户（如图2-3）。\n\n![img](6-1-my-paper/wps4.jpg) \n\n图 2-3 基于物品的协同推荐系统\n\n### 2.1.3 混合推荐算法\n\n混合推荐算法指的是将多种推荐算法结合起来使用，以提高推荐质量和准确性的一种方法。常见的组合方式有以下几种：\n\n(1)基于规则的组合：根据一定的规则，如用户的历史行为、商品的属性等，将多种算法的结果进行组合并加权，然后进行推荐。\n\n(2)基于特征的组合：将用户和商品的特征进行学习，并结合多种推荐算法进行预测和推荐。\n\n(3)基于模型的组合：将多种算法分别训练出来的模型进行融合，以得到更准确的推荐结果。\n\n(4)基于内容的组合：将基于内容的推荐算法与其他推荐算法结合使用，使用内容信息补充推荐模型缺失的信息，以提高推荐效果。\n\n![img](6-1-my-paper/wps5.jpg) \n\n图 2-4 不同情形之下可行推荐策略\n\n混合推荐算法具有较高的灵活性和适应性，并能够在不同的场景中得到应用。但是，也存在难以选择合适的组合方式和模型参数等问题。通常当用户请求推荐服务时，推荐接口服务先获取用户的协同过滤推荐结果，如果没有，再去取用户的基于内容的推荐结果，如果还没有，最后就用户热门默认推荐（如图2-4）。\n\n## 2.2图神经网络\n\n图神经网络(GNN)是一种用于处理图数据的神经网络。它的出现是为了解决传统神经网络不能处理非欧几里得结构数据的问题。图神经网络主要是用于解决节点分类、边分类、图分类等问题。它是由一系列基本的图神经网络模块组成的，包括图卷积神经网络(GCN)、图注意力网络(Graph Attention Network, GAT)以及基于图卷积拉普拉斯算子的谱卷积神经网络(Spectral Convolutional Neural Network, SCNN)等。\n\n图神经网络能够很好地捕捉图中节点和边之间的关系，具有良好的可解释性和泛化能力。因此，在社交网络分析、化学分子分析、图像分割等领域得到了广泛的应用。\n\n### 2.2.1 图卷积神经网络\n\n图卷积神经网络( GCN)是一种基于卷积操作的图神经网络，它可以用于学习节点在具有图结构的数据集上的表示，其网络结构如图2-5。它的基本思想是将节点的邻居节点信息进行卷积操作，获得每个节点的新特征表示，从而实现节点分类、链接预测等任务。\n\n![img](6-1-my-paper/wps6.jpg) \n\n图 2-5 图卷积网络模型\n\n\n\n对于一个图G，其中有M个节点，每个节点都有自己的特征，设这些节点的特征组成一个M×D维的矩阵A，然后各个节点之间的关系也会形成一个N×N维的矩阵X，也称为邻接矩阵。X和A便是模型的输入。GCN也是一个神经网络层，它的层与层之间的传播方式是：\n\n|      | ![img](6-1-my-paper/wps7.jpg) | (2-1) |\n| ---- | :---------------------------: | ----: |\n\n上式中，σ(·)是sigmoid函数，用于非线性激活；![img](6-1-my-paper/wps8.jpg)=![img](6-1-my-paper/wps9.jpg)+I，I是单位矩阵；![img](6-1-my-paper/wps10.jpg)是![img](6-1-my-paper/wps11.jpg)的度矩阵；H是每一层的特征，对于输入层，H就是A。\n\nGCN的学习过程需要通过多层卷积操作来逐步增强每个节点的邻居节点信息。在每一层中，GCN会先对节点及其邻居节点构成的局部子图进行卷积，然后使用池化或者其他操作将图缩小以进一步改善模型的效率。这种多层卷积的过程类似于深度神经网络中的多层感知机，因此，GCN也被称为图卷积多层感知机(Graph Convolutional Multi-Layer Perceptron, GC-MPL)。\n\nGCN被广泛应用于社交网络分析、图像数据处理、化学分子分析等领域，在不同的任务上都取得了良好的效果。\n\n### 2.2.2 图注意力网络\n\n图注意力网络(GAT)是一种基于注意力机制的图神经网络，它能够对每个节点的邻居节点信息进行加权聚合，从而捕捉节点和邻居之间更为精细、动态的关系。相对于传统的图卷积神经网络(GCN)，GAT具有更强的自适应能力和灵活性。\n\n在GAT中，每个节点的邻居节点信息会被映射到一个低维空间中，然后使用注意力权重来调节每个邻居节点的重要性，这些邻居节点的特征向量经过加权求和后，与该节点本身的特征向量一同输入到后续的神经网络层。这种注意力机制使得GAT能够自适应地对邻居节点进行加权，获得更为准确的节点表示。\n\nGAT 中的注意力机制可以通过学习得到，也可以事先设定为固定的形式。对于不同任务和数据集，我们可以选择不同的注意力机制来获得最佳的性能。GAT已经被证明在很多领域取得了优越的表现，包括节点分类、链接预测、社区发现等任务。\n\n## 2.3注意力机制\n\n注意力机制是一种人工智能算法，其基本思想是在模型中使用一个可学习的权重分配机制，使其能够将注意力集中在输入序列的某些部分上，以便提高模型的精度和准确性。\n\n![img](6-1-my-paper/wps12.jpg) \n\n图 2- 6  注意力机制模型\n\n注意力机制可抽象为一种通用的不依赖于具体框架的思想，其核心目标是从众多信息中选择出对当前任务目标更关键的信息。Encoder-Decoder框架的注意力模型，编码器使用RNN、CNN[38]等神经网络提取输入序列的特征，计算得到编码器的隐状态hi，而注意力模型中的上下文向量已不再是共享同一个上下文向量c，而是不同的时间步t使用不同的上下文向量ct。注意力机制模型如图2-6。由图可知，上下文向量ct可以关注到输入中最相关的部分，也就是“注意力”的计算式如下：\n\n|      | ![img](6-1-my-paper/wps13.jpg) | (2-1) |\n| ---- | :----------------------------: | ----: |\n\n如2-4式所示，在具体实现时，注意力机制常常采用“加权和”方式对输入序列进行加权计算。这些权重由激活函数产生，取决于输入序列中的每个元素和当前输出单元之间的相似程度。在每个时间步骤中，模型会基于一组权重对输入序列进行加权求和，以计算输出序列中的当前元素。\n\n \n\n \n\n \n\n\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n# 3 基于图神经网络和注意力机制的社交推荐算法研究\n\n现有的深度神经网络模型的推荐系统主要有基于社交网络图的DiffNet[39]、DiffNet++[40]和基于图形嵌入的轻量级推荐算法模型的方法 LightGCN[11] 。其中其中较为轻量的模型 （如LightGCN[11] ）未考虑用户之间关系网络导致数据稀缺；一些基于社交关系网络图的模型（如DiffNet[39]）未将注意力引入模型，未能充分挖掘数据的信息；一些在社交关系网络中引入了注意力机制的模型（如DiffNet++[40]）虽然考虑了用户社交关系网和图卷积网络并将注意力机制引入模型，但是存在参数冗余的问题，导致在复杂的社交关系网络中计算非常耗时费力。\n\n针对以上方法的缺陷，本文提出了一种基于图神经网络和注意力机制的社交推荐算法研究，本算法结合了图卷积神经网络、注意力机制等技术，并且利用利用社交网络数据来缓解数据稀疏性，通过上下文交互信息获取用户和项目当前相关信息的嵌入表示；然后在社交关系图和用户交互图卷积；接着融合注意力机制添加权重来区分不同尾实体在语境下的语义信息，最后在此基础上将用户和项目的表示向量聚合内积得到预测评分，并使用设置损失函数对模型进行学习。 \n\n具体的推荐模型结构如图3-1所示，模型框架可分为四部分：1）嵌入表示层；2）基于用户交互与社交网络的图卷积网络层；3）基于注意力机制的图聚合学习层；4）预测推荐层。下面将把模型这四部分结构详细阐明。\n\n \n\n![img](6-1-my-paper/wps14.jpg) \n\n图 3-1  模型结构图\n\n## 3.1 嵌入表示层\n\n模型根据主流推荐模型，用嵌入向量eu∈Rd描述一个用户u，用ei∈Rd描述一个项目i，其中d表示嵌入大小，如图3-2。这可以看作是构建一个参数矩阵作为一个嵌入的查找表E：\n\n|      | ![img](6-1-my-paper/wps15.jpg) | *3-1) |\n| ---- | :----------------------------: | ----: |\n\n![img](6-1-my-paper/wps16.jpg) \n\n图 3-2  用户和项目的嵌入表示\n\n \n\n## 3.2 基于用户交互与社交网络的图卷积网络\n\n### 3.2.1 用户-用户社交图卷积网络\n\n每个用户都对应一个固定的嵌入![img](6-1-my-paper/wps17.jpg)（即一组神经网络的权重)，表示该用户在整个社交网络中的潜在偏好。这些嵌入将作为网络的输入，用以引导社交网络中的信息扩散，模型如图3-3。\n\n![img](6-1-my-paper/wps18.jpg) \n\n图 3-3  用户社交图卷积模型\n\n \n\n该模型的输入是一个动态社会网络S，其中包含多个用户，即用户u有社交关系的用户集为![img](6-1-my-paper/wps19.jpg)。第n层将用户来自前一层的嵌入作为输入，并在当前的社会扩散过程完成后输出用户更新后的嵌入，然后更新后的用户嵌入被发送到第n+1层进行下一个扩散过程。\n\n|      | ![img](6-1-my-paper/wps20.jpg) | (3-2) |\n| ---- | :----------------------------: | ----: |\n\n其中Pool函数可以定义为一个平均池，它对所有邻居用户在第n层的潜在嵌入执行一个平均操作。该函数也可以定义为一个最大操作，选择所有邻居用户在第k层的潜在嵌入的最大元素，形成![img](6-1-my-paper/wps21.jpg)，卷积网络细节如图3-4。\n\n![img](6-1-my-paper/wps22.jpg) \n\n图 3-4 用户社交图卷积层细节\n\n### 3.2.2 用户-项目交互图卷积网络\n\n在用户嵌入和项目嵌入组成的二部图中，设置项目i有交互过的用户集为![img](6-1-my-paper/wps23.jpg)。同理，用户u有交互过的项目集为![img](6-1-my-paper/wps24.jpg)。每层图卷积网络的输入为用户嵌入eu（或项目嵌入ei）,按照其有交互的项目（用户）的数目，计算出更新过后的用户嵌入eu’(或项目嵌入ei’）并输出，卷积网络模型如图3-5。\n\n![img](6-1-my-paper/wps25.jpg) \n\n图 3-5 用户-项目交互图卷积模型\n\n为了采用简单加权和聚合器，不再使用特征变换和非线性激活。本模型中的图卷积运算定义为：\n\n|      | ![img](6-1-my-paper/wps26.jpg) | (3-3) |\n| ---- | :----------------------------: | ----: |\n|      | ![img](6-1-my-paper/wps27.jpg) | (3-4) |\n\n其中对称归一化项![img](6-1-my-paper/wps28.jpg)遵循标准GCN 的设计，它避免了嵌入规模随图卷积运算的增加，卷积网络细节如图3-6。\n\n![img](6-1-my-paper/wps29.jpg) \n\n图 3-6 用户-项目交互图卷积层细节\n\n \n\n## 3.3 基于注意力机制的图聚合学习\n\n为了实现自动优化注意力权重，模型设计了一个图形注意网络来自动学习图形集成的参数，如图3-7。\n\n![img](6-1-my-paper/wps30.jpg) \n\n图 3-7 本模型的注意力机制\n\n模型建模的图的注意力权重为![img](6-1-my-paper/wps31.jpg)和![img](6-1-my-paper/wps32.jpg)（d’是注意力的大小）为：\n\n|      | ![img](6-1-my-paper/wps33.jpg) | (3-5) |\n| ---- | :----------------------------: | ----: |\n|      | ![img](6-1-my-paper/wps34.jpg) | (3-6) |\n\n其中![img](6-1-my-paper/wps35.jpg)和![img](6-1-my-paper/wps36.jpg)表示两个图的注意权重，W1∈Rd’×2d这是可训练的权重矩阵，[·]是连接操作。之后，用以式(3-7)对注意力权重进行归一化处理：\n\n|      | ![img](6-1-my-paper/wps37.jpg) | (3-7) |\n| ---- | :----------------------------: | ----: |\n|      | ![img](6-1-my-paper/wps38.jpg) | (3-8) |\n\n其中sum（·）表示注意力权重的和，那么以同理，可以得到![img](6-1-my-paper/wps39.jpg)。因为![img](6-1-my-paper/wps40.jpg)+![img](6-1-my-paper/wps41.jpg) = 1，更大![img](6-1-my-paper/wps42.jpg)意味着社交关系网络对模型结果的影响占比更大。当然，每一层的注意值是按照当时的嵌入计算出，实现了自动优化权重。\n\n注意力层最终输出的用户嵌入为：\n\n|      | ![img](6-1-my-paper/wps43.jpg) | (3-9) |\n| ---- | :----------------------------: | ----: |\n\n## 3.4 预测推荐层\n\n对于模型输出的结果用聚合器聚合用户和项目的表示，可以得到一阶和高阶语义表示对项目关联性和用户偏好的影响。计算用户和项目向量内积，预测用户偏好，实现推荐。常用的聚合方式为加和聚合器和连接聚会器。\n\n加和聚合器：表示对多个不同层向量的加和（K为网络层数）：\n\n|      | ![img](6-1-my-paper/wps44.jpg) | (3-10) |\n| ---- | :----------------------------: | -----: |\n|      | ![img](6-1-my-paper/wps45.jpg) | (3-11) |\n\n连接聚合器：表示将多个不同层向量连接起来（K为网络层数）：\n\n|      | ![img](6-1-my-paper/wps46.jpg) | (3-12) |\n| ---- | :----------------------------: | -----: |\n|      | ![img](6-1-my-paper/wps47.jpg) | (3-13) |\n\n最后用![img](6-1-my-paper/wps48.jpg)、![img](6-1-my-paper/wps49.jpg)分别表示用户、物品的聚合后的表示矩阵，将两个聚合表示矩阵进行矩阵相乘，获得用户对项目的偏好预测值，如图3-8：\n\n![img](6-1-my-paper/wps50.jpg) \n\n图 3-8嵌入相乘预测推荐\n\n|      | ![img](6-1-my-paper/wps51.jpg) | (3-14) |\n| ---- | :----------------------------: | -----: |\n\n## 3.5 损失函数\n\n为了关注用户的隐式反馈，本模型选择了BPR 损失函数进行优化：\n\n|      | ![img](6-1-my-paper/wps52.png) | (3-15) |\n| ---- | :----------------------------: | -----: |\n\n其中![img](6-1-my-paper/wps53.jpg)是一个sigmoid 型函数。Θ = [Θ1, Θ2]，带有Θ1 = [P, Q]，和Θ2 =[F,![img](6-1-my-paper/wps54.jpg)]。λ是一个正则化参数，控制用户和项目自由嵌入的复杂性矩阵。Da= {(i, j)|i∈Ra∧j∈V−Ra }表示a与Ra的两两训练数据表示一个积极显示反馈的项目集。\n\n \n\n \n\n \n\n# 4  实验设置与结果评估\n\n在这一章节中，本文将利用公开数据集LastFM、Yelp 和Douban-book模拟一种真实的推荐场景来具体研究以下三个问题： 1）本课题模型与其他推荐模型（DiffNet[39]、DiffNet++[40]和LightGCN[11] ）相比效率；2）不同参数（模型层数、聚合器、嵌入维度）的设置将如何影响最终的推荐结果；3）对本模型进行注意力机制消融实验将如何影响最终的推荐结果。\n\n## 4.1实验环境\n\n4.1.1硬件环境\n\n1）处理器：Intel(R)Core(TM)i7-8565UCPU@1.80GHz1.99GHz\n\n2）显卡：NVIDIAGeForceMX150\n\n3）机带RAM：8.00GB(7.85GB可用)\n\n4.1.2软件环境\n\n1）Windows11家庭中文版（版本：22H2）\n\n2）Python3.9.10\n\n3）PyCharmCommunityEdition2022.1.164位\n\n4）Torch1.12.1\n\n5）Torchvision0.14.1+cu116\n\n6）Numpy1.21.6\n\n7）Scikit-Learn1.0.2\n\n## 4.2数据集\n\n本文选择在三个数据集上进行了实验：LastFM、Yelp 和Douban-book。LastFM是一个音乐社交平台，用户可通过它来发现新音乐、跟踪已知艺人，并与其他音乐爱好者进行互动。Yelp是一个跨平台的用户评论网站，主要提供有关当地商家的信息、用户评论和排名，旨在帮助用户更方便地寻找商家和服务。Douban-book是一个在线书籍社区，用户可以在其中记录自己读过的书、给书评分、撰写书评、参加书籍讨论等。其中Douban-book由 1-5 级的评分组成。 将豆瓣书中的评分小于或等于3，并将其余设置为1，表示隐式反馈数据。 表 4-1 总结了数据集的特征。 没有交互的用户在社交网络中被过滤掉。\n\n表 4-1 实验三个数据集的特征\n\n|             | 用户数 | 项目数 | 交互数 | 关系数 |  密度  |\n| :---------: | :----: | :----: | :----: | :----: | :----: |\n|   Last.fm   |  1892  | 17632  | 92834  | 25434  | 0.288% |\n|    Yelp     | 17237  | 38342  | 204448 | 143765 | 0.033% |\n| Douban-book | 13024  | 22347  | 792062 | 169150 | 0.272% |\n\n## 4.3评估指标\n\n### 4.3.1召回率(Recall)\n\n召回率(Recall)是衡量推荐算法召回用户历史兴趣的能力的一种重要指标。具体而言，召回率是指在所有用户实际感兴趣的物品中，推荐系统成功推荐的物品占比。在模型预测结果中，设置TP(True Positive) : 表示样本的真实类别为正,最后预测得到的结果也为正，FN(False Negative): 表示样本的真实类别为正,最后预测得到的结果为负，Recall的公式如下：\n\n|      | ![img](6-1-my-paper/wps55.jpg) | (4-1) |\n| ---- | :----------------------------: | ----: |\n\nRecall@K表示结果列表前K项的Recall，随着K的变化，可以得到一系列的Recall值。Recall@K的公式如下：\n\n|      | ![img](6-1-my-paper/wps56.jpg) | (4-2) |\n| ---- | :----------------------------: | ----: |\n\n其中TP@K指的是在推荐结果的前K个物品中，与用户兴趣相关的物品数量，即真正例的数量。FN@K指的是所有与用户兴趣相关的物品数量减去在前K个推荐结果中与用户兴趣相关的物品数量。\n\n### 4.3.2归一化折损累计增益(NDCG)\n\n归一化折损累计增益(NDCG)是衡量推荐系统中排序质量的一种指标。NDCG是DCG（折损累计增益）的归一化处理，用来衡量推荐算法排序算法对于用户兴趣的预测质量。用户u上的nDCG![img](6-1-my-paper/wps57.jpg)为：\n\n|      | ![img](6-1-my-paper/wps58.jpg) | (4-3) |\n| ---- | :----------------------------: | ----: |\n\n其中用户u的DCG![img](6-1-my-paper/wps59.jpg)的计算公式为：\n\n|      | ![img](6-1-my-paper/wps60.jpg) | (4-4) |\n| ---- | :----------------------------: | ----: |\n\n其中reli表示处于位置i的推荐结果的相关性，K表示要考察的推荐列表的大小。式（4-3）中用户u的IDCG为理想的DCG的计算公式为：\n\n|      | ![img](6-1-my-paper/wps61.jpg) | (4-5) |\n| ---- | :----------------------------: | ----: |\n\n## 4.4其他对比模型\n\nDiffNet[39]是一种用于推荐系统的深度神经网络模型，其灵感来源于传统的协同过滤推荐方法。它利用图形卷积神经网络来学习用户-项目（或者用户-用户）之间的相似性，其主要思想是通过学习一个项目（或用户）嵌入向量，然后在项目（用户）图上进行卷积操作来计算项目（或用户）之间的相似性，并将其用于推荐系统中。\n\nDiffNet++[40]是DiffNet[39]的改进版本，它在DiffNet[39]的基础上增加了时间因素和用户的行为模式。\n\nLightGCN[11] 是一种基于图形嵌入的轻量级推荐算法模型，是利用图嵌入技术从推荐中提取用户和物品之间的相似性，从而为用户提供个性化推荐列表。\n\n## 4.5实验设置\n\n​    在实验过程中，本课题将数据集训练集、测试集和验证集按照8:1:1划分。在对比实验方面，本课题使用召回率(Recall)、归一化折损累计增益(NDCG) 来评估个方法的预测效果。在参数影响实验方面，本课题控制变量对模型层数、聚合器种类、嵌入维度和消融注意力机制实验进行测试，探究各参数对实验结果的影响并进行分析。\n\n## 4.6对照实验结果\n\n本论文提出的模型在LastFM、Yelp 和Douban-book三个数据集下的Recall@10值和NDCG@10值方面都表现出了领先的结果，如表4-2。\n\n表 4-2 对照实验结果\n\n|           | Last.fm   |         | Yelp      |         | Douban-book |         |\n| --------- | --------- | ------- | --------- | ------- | ----------- | ------- |\n|           | Recall@10 | NDCG@10 | Recall@10 | NDCG@10 | Recall@10   | NDCG@10 |\n| 本模型    | 0.4434    | 0.3796  | 0.3853    | 0.3134  | 0.4012      | 0.3387  |\n| DiffNet   | 0.4216    | 0.3598  | 0.3687    | 0.2873  | 0.3919      | 0.3024  |\n| DiffNet++ | 0.4323    | 0.3748  | 0.3808    | 0.3022  | 0.4005      | 0.3293  |\n| LightGCN  | 0.4298    | 0.3539  | 0.3729    | 0.2885  | 0.3885      | 0.3124  |\n\n在Yelp数据集中，本论文模型的Recall@10值显著高于DiffNet[39]、DiffNet++[40]和LightGCN[11] 。具体而言，本论文模型的Recall@10值为0.4434，而DiffNet[39]、DiffNet++[40]和LightGCN[11] 的Recal@10值分别为0.4216、0.4323和0.4298。此外，本论文模型在Yelp数据集中的NDCG@10值也达到了极高水平，为0.3796，远高于其他三个模型。\n\n在Flickr数据集中，本论文模型的表现同样突出。本论文模型的Recall@10值为0.3853，远高于DiffNet[39]、DiffNet++[40]和LightGCN[11] 的值，分别为0.3687、0.3808和0.3729。此外，本论文模型的NDCG@10值也得到了显著提升，为0.3134。\n\n在Last.fm数据集中，本论文模型的表现也相当出色。本论文模型的Recall@10值为0.3387，略高于DiffNet[39]、DiffNet++[40]和LightGCN[11] 的Recal@10l值，为0.4012。与此同时，本论文模型在Last.fm数据集中的NDCG@10值也达到了0.3387，远高于其他三个模型。\n\n综合以上结果，本论文提出的模型在三个数据集下的表现都达到了较高水平，具有显著的优势和潜力，值得在实际推荐场景中进一步探索和应用，如图4-1。\n\n|   ![img](6-1-my-paper/wps62.jpg) (a) Last.fm数据集对比实验   |\n| :----------------------------------------------------------: |\n|   ![img](6-1-my-paper/wps63.jpg) (b)  Yelp数据集的对比实验   |\n| ![img](6-1-my-paper/wps64.jpg) (c) Douban-book数据集的对比实验 |\n\n图 4-1 模型在不同top-K推荐下的召回率\n\n## 4.7参照对实验结果分析影响\n\n本文还分析了网络层数、嵌入维度等参数以及不同聚合器对模型性能的影响，下面主要给出了在Yelp数据集中的实验情况。\n\n### 4.7.1模型网络层数对实验结果的影响\n\n在社交推荐模型中，网络层数通常会影响实验结果。当设置不同的网络层数模型预测结果如图4-2所示。显然，网络层数的适当设定可以提高模型的效果，但如果网络层数过多，模型可能会出现过拟合现象。\n\n![img](6-1-my-paper/wps65.jpg) \n\n图 4-2 不同网络层数模型的召回率\n\n### 4.7.2嵌入维度对实验结果的影响\n\n嵌入维度是推荐系统中一个非常重要的超参数，它可以影响到推荐系统实验的结果。现为了探究该参数对于本模型的预测结果的影响，改变模型向量的嵌入深度的大小分别为8，16，32，64和128，观察嵌入深度为这些值的时Recall@10的大小。实验结果如图4-3所示，由图可知当嵌入维度为64时，模型的预测结果最佳。\n\n \n\n![img](6-1-my-paper/wps66.jpg) \n\n图 4-3 不同嵌入维度模型的召回率\n\n### 4.7.3聚合器对实验结果的影响\n\n本文3.4中提到的，聚合器的两种主要的实现方式亦会对模型性能产生影响。现改变聚合器的的种类，探究不同聚合器对实验结果的影响。如图4-4两种聚合器中，加和聚合器对于本模型预测结果优于拼接聚合器。\n\n![img](6-1-my-paper/wps67.jpg) \n\n图 4-4 不同聚合器模型的召回率\n\n### 4.7.4注意力机制的消融实验\n\n为了进一步说明注意力机制在模型中的重要性或作用，通过对比有无注意力机制的模型实验结果，评估注意力机制对推荐算法性能的影响。本实验在实现的推荐算法中，分别使用了带有注意力机制的模型和不带注意力机制的基准模型进行训练和测试。根据Recall@10、NDCG@10进行评估，看两个模型差异的显著性。消融实验结果如图4-5。\n\n显然，模型中加入注意力机制可以显著提高模型预测的性能。\n\n![img](6-1-my-paper/wps68.jpg) \n\n图4-5 注意力机制的消融实验结果\n\n \n\n\n\n \n\n \n\n# 结论\n\n \n\n本篇论文通过研究并掌握多种融合注意力机制和图神经网络的社交推荐模型，结合图神经网络技术对传统协同过滤算法中的冷启动及稀疏性问题进行深入探究，提出了一种基于图神经网络和注意力机制的社交推荐算法，通过对用户历史交互行为进行分析，利用获取的用户及项目特征融合注意力机制进行推荐，并利用LastFM、Yelp 和Douban-book的真实数据集进行了模拟测试与比较。本算法相比于现有的推荐算法(DiffNet[39])存在着较大的优势，即提高了推荐的准确性和召回率。\n\n但是，本研究也存在一些不足，例如该算法还存在推荐瓶颈问题，对于大型数据集本模型运算通常需要大量的计算和存储资源。未来将探索更加高效的算法和大规模分布式计算技术等。此外，当前该算法只考虑了静态数据集，未来希望将时序信息和多模态数据纳入模型中，进一步提高推荐算法的效果。\n\n综上所述，本文致力于研究并构建了基于图神经网络和注意力机制的社交推荐算法，通过实验证明其在推荐准确性和召回率方面的有效性。其中所提出的思路及相应的技术手段也有望涉及到其他领域的研究方向。最后，相信随着未来研究的深入，在更多的场景下可以更好地利用图神经网络和注意力机制提高推荐效果，为社交推荐算法的发展做出更大的贡献。\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n# 致谢\n\n \n\n在完成本篇论文期间，我收获了许多宝贵的经历和收获，这些经验和收获离不开许多人的帮助和支持。因此，我要特别感谢以下两位尤其对本研究的支持和帮助：\n\n首先，我要向我的毕设导师孟顺梅老师表达最深切的谢意。孟老师一直以来都是我学术上的指导和榜样。她一直在培养我在研究中学习技能，并支持我进行独立的学术研究。她的鼓励、支持和指导是让我选择本课题的重要原因，没有她的支持，我将无法完成这篇论文。我要特别感谢孟老师对我的悉心指导和无私支持。\n\n其次，我要向庄嘉博学长表达我的感谢之情。他在我完成本论文期间提供了无私的帮助和支持。在我遇到困难和难题时，他总是耐心地听我倾诉，并给我提供最及时和最准确的建议和指导。他的经验和专业知识非常之丰富，帮助我理解研究中的许多难点。在本论文的研究过程中，他对我学术上的支持是至关重要的，我再次感谢他的无私帮助。\n\n最后，我还想感谢家人和朋友对我在完成整个论文过程中的支持，他们是我不懈努力的动力源泉。感谢热心的同学在本科期间给予的帮助和支持。我也要感谢论文中的研究对象和数据源，没有它们的支持，我的研究将无从谈起。\n\n在此，我再次向孟顺梅老师和庄嘉博学长表达我的最深刻的感谢之情，他们的支持和帮助是我研究成果的重要功臣。\n\n \n\n \n\n\n\n \n\n \n\n# 参考文献\n\n \n\n[1] Silva N B, Tsang R, Cavalcanti G D C, et al. A graph-based friend recommendation system using genetic algorithm[C]//IEEE congress on evolutionary computation. IEEE, 2010: 1-7.\n\n[2] Hamilton W, Ying Z, Leskovec J. Inductive representation learning on large graphs[J]. Advances in neural information processing systems, 2017, 30. \n\n[3] Ji S, Pan S, Cambria E, et al. A survey on knowledge graphs: Representation, acquisition, and applications[J]. IEEE transactions on neural networks and learning systems, 2021, 33(2): 494-514.\n\n[4] Shams B, Haratizadeh S. Graph-based collaborative ranking[J]. Expert Systems with Applications, 2017, 67: 59-70.\n\n[5] Shi C, Kong X, Huang Y, et al. Hetesim: A general framework for relevance measure in heterogeneous networks[J]. IEEE Transactions on Knowledge and Data Engineering, 2014, 26(10): 2479-2492.\n\n[6] Musto C, Basile P, Lops P, et al. Introducing linked open data in graph-based recommender systems[J]. Information Processing & Management, 2017, 53(2): 405-435.\n\n[7] Palumbo E, Rizzo G, Troncy R, et al. Knowledge graph embeddings with node2vec for item recommendation[C]//The Semantic Web: ESWC 2018 Satellite Events: ESWC 2018 Satellite Events, Heraklion, Crete, Greece, June 3-7, 2018, Revised Selected Papers 15. Springer International Publishing, 2018: 117-120.\n\n[8] Fan W, Ma Y, Li Q, et al. Graph neural networks for social recommendation[C]//The world wide web conference. 2019: 417-426.\n\n[9] Gao Y, Li Y F, Lin Y, et al. Deep learning on knowledge graph for recommender system: A survey[J]. arXiv preprint arXiv:2004.00387, 2020.\n\n[10] Ying R, He R, Chen K, et al. Graph convolutional neural networks for web-scale recommender systems[C]//Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 2018: 974-983.\n\n[11] He X, Deng K, Wang X, et al. Lightgcn: Simplifying and powering graph convolution network for recommendation[C]//Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 2020: 639-648.\n\n[12] Monti F, Boscaini D, Masci J, et al. Geometric deep learning on graphs and manifolds using mixture model cnns[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 5115-5124. \n\n[13] Defferrard M, Bresson X, Vandergheynst P. Convolutional neural networks on graphs with fast localized spectral filtering[J]. Advances in neural information processing systems, 2016, 29.\n\n[14] Berg R, Kipf T N, Welling M. Graph convolutional matrix completion[J]. arXiv preprint arXiv:1706.02263, 2017.\n\n[15] Fan W, Ma Y, Li Q, et al. Graph neural networks for social recommendation[C]//The world wide web conference. 2019: 417-426.\n\n[16] Zhang M, Chen Y. Inductive matrix completion based on graph neural networks[J]. arXiv preprint arXiv:1904.12058, 2019.\n\n[17] Sun J, Zhang Y, Ma C, et al. Multi-graph convolution collaborative filtering[C]//2019 IEEE international conference on data mining (ICDM). IEEE, 2019: 1306-1311. \n\n[18] Wang X, Jin H, Zhang A, et al. Disentangled graph collaborative filtering[C]//Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval. 2020: 1001-1010.\n\n[19] Zhong T, Zhang S, Zhou F, et al. Hybrid graph convolutional networks with multi-head attention for location recommendation[J]. World Wide Web, 2020, 23: 3125-3151.\n\n[20] Hekmatfar T, Haratizadeh S, Goliaei S. Embedding ranking-oriented recommender system graphs[J]. Expert Systems with Applications, 2021, 181: 115108.\n\n[21] Duran P G, Karatzoglou A, Vitria J, et al. Graph convolutional embeddings for recommender systems[J]. IEEE Access, 2021, 9: 100173-100184.\n\n[22] Song J, Chang C, Sun F, et al. Graph attention collaborative similarity embedding for recommender system[C]//Database Systems for Advanced Applications: 26th International Conference, DASFAA 2021, Taipei, Taiwan, April 11–14, 2021, Proceedings, Part III 26. Springer International Publishing, 2021: 165-178.\n\n[23] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.\n\n[24] Veličković P, Cucurull G, Casanova A, et al. Graph attention networks[J]. arXiv preprint arXiv:1710.10903, 2017.\n\n[25] Chen F, Pan S, Jiang J, et al. DAGCN: dual attention graph convolutional networks[C]//2019 International Joint Conference on Neural Networks (IJCNN). IEEE, 2019: 1-8.\n\n[26] Hong H, Guo H, Lin Y, et al. An attention-based graph neural network for heterogeneous structural learning[C]//Proceedings of the AAAI conference on artificial intelligence. 2020, 34(04): 4132-4139.\n\n[27] Zhang Z, Zhuang F, Zhu H, et al. Relational graph neural network with hierarchical attention for knowledge graph completion[C]//Proceedings of the AAAI conference on artificial intelligence. 2020, 34(05): 9612-9619.  \n\n[28] Chang H, Rong Y, Xu T, et al. Spectral graph attention network with fast eigen-approximation[C]//Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 2021: 2905-2909.\n\n[29] Wu Q, Zhang H, Gao X, et al. Dual graph attention networks for deep latent representation of multifaceted social effects in recommender systems[C]//The world wide web conference. 2019: 2091-2102.\n\n[30] Ashish V. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30: I.\n\n[31] Cheng Z, Ding Y, He X, et al. A^ 3NCF: An Adaptive Aspect Attention Model for Rating Prediction[C]//IJCAI. 2018: 3748-3754.[32] PEI W J，YANG J，SUN Z，et al. Interacting attention-gated recurrent networks for recommendation[C]  //Proceedings of the 2017 ACM on Conference on Information and Knowledge Management，Singapore， Nov 6-Nov 10，2017. New York: ACM，2017: 1459–1468. \n\n[33] 柴玉梅, 员武莲, 王黎明, 等. 基于双注意力机制和迁移学习的跨领域推荐模型[J]. 计算机学报, 2020, 43(10): 1924-1942.\n\n[34] Chen J, Zhang H, He X, et al. Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention[C]//Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. 2017: 335-344. \n\n[35] Guo X, Zhu J H. Deep Neural Network Recommendation ModelBasedon UserVectorizationRepresentationand AttenG tion Mechanism[J]. ComputerScience, 2019, 46(8): 111G115.\n\n[36] Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907, 2016.\n\n[37] Maas A L, Hannun A Y, Ng A Y. Rectifier nonlinearities improve neural network acoustic models[C]//Proc. icml. 2013, 30(1): 3.\n\n[38] 周飞燕, 金林鹏, 董军. 卷积神经网络研究综述[J]. 计算机学报, 2017, 40(6): 1229-1251.\n\n[39] Wu L, Sun P, Fu Y, et al. A neural influence diffusion model for social recommendation[C]//Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval. 2019: 235-244.\n\n[40] Wu L, Li J, Sun P, et al. Diffnet++: A neural influence and interest diffusion network for social recommendation[J]. IEEE Transactions on Knowledge and Data Engineering, 2020, 34(10): 4753-4766.\n\n \n","tags":["private"],"categories":["paper"]},{"title":"创建新的Hexo-Blog主题构想","url":"/2023/05/19/5-19-博客主题构想/","content":"\n- [ HOME](#head1)\n- [ BLOG](#head2)\n- [ RESUME](#head3)\n- [ HOBBY](#head4)\n- [ DIARY](#head5)\n- [ REVIEW](#head6)\n- [ PREVIEW](#head7)\n\n# <span id=\"head1\"> HOME</span>\n\n![img.png](5-19-博客主题构想/img.png)\n\n- 可以到达六个模块\n- 左上角可以转换四种语言\n- 右上角连接到GitHub账号\n\n# <span id=\"head2\"> BLOG</span>\n\n![img_1.png](5-19-博客主题构想/img_1.png)\n\n- 左部：按照类别和目录归纳，左上角是BLOG首页\n- 中部：博客内容，左上角是类别，右上角是修改时间，底部是评论区\n- 右部：文章toc，右上角是HOME页链接\n\n# <span id=\"head3\"> RESUME</span>\n\n![img_2.png](5-19-博客主题构想/img_2.png)\n\n- 左部：左上角是RESUME首页，左下角转换四种语言\n- 中部：简历\n- 右部：简历toc，右上角是HOME页链接\n\n# <span id=\"head4\"> HOBBY</span>\n\n![img_3.png](5-19-博客主题构想/img_3.png)\n\n- 左部：按照类别和目录归纳，左上角是HOBBY首页\n- 中部：内容，左上角是类别，右上角是时间戳\n- 右部：内容toc，右上角是HOME页链接\n\n# <span id=\"head5\"> DIARY</span>\n\n![img_4.png](5-19-博客主题构想/img_4.png)\n\n- 左部：日期+题目，左上角是DIARY首页\n- 中部：内容，左上角是今日心情、新日天气，右上角是时间戳\n- 右部：本月心情记录，本月天气记录，右上角是HOME页链接\n\n# <span id=\"head6\"> REVIEW</span>\n![img_5.png](5-19-博客主题构想/img_5.png)\n\n\n- 左部：年、月、日归纳，点击年有年复盘，点击月有月复盘，点击日是日复盘，左上角是REVIEW首页\n- 中部：\n  - 日复盘：按日复盘，左上角是第几周周几，右上角是时间戳\n  - 月复盘：按月复盘，左上角是几月，右上角是时间戳\n  - 年复盘：按年复盘，左上角是几年，右上角是时间戳\n- 右部：在日历上，按照周次和周几复盘，点击第几周是周复盘，右上角是HOME页链接\n\n# <span id=\"head7\"> PREVIEW</span>\n\n![img_6.png](5-19-博客主题构想/img_6.png)\n\n- 左部：按照规划类型归纳，左上角是PREVIEW首页\n- 中部：\n- 右部：内容toc，右上角是HOME页链接\n\n","tags":["hobby","private"],"categories":["thoughts"]},{"title":"Array 3:Squares of a Sorted Array","url":"/2023/05/14/5-14-array-3-squares-of-a-sorted-array/","content":"- [977. 有序数组的平方](#head1)\n    - [ NOTE](#head2)\n    - [c++ solution](#head3)\n    - [java solution](#head4)\n    - [improved solution](#head5)\n\n# <span id=\"head1\">[977. 有序数组的平方](https://leetcode.cn/problems/squares-of-a-sorted-array/description/)</span>\n\n![977](5-14-array-3-squares-of-a-sorted-array/img.png)\n\n## <span id=\"head2\"> NOTE</span>\n\n原本自己读完题想着要先挨个平方完，然后快排（没错本人就是这么垃圾:sob:）这样的话，是复杂度是O(n+nlogn)，\n绝对超过了O(n)。之后看到了提示说用双指针，就恍然大悟：①遍历数组找到正负分界点②从这个点向两端依次确定\n最小值，直到有一侧达到尽头③处理剩下的数组。写完之后，发现卡哥的思路：从数组两端向中间聚拢，以此确定最大值，\n这样省了找分界点和处理剩下元素的时间，respect！！\n\n## <span id=\"head3\">c++ solution</span>\n\n```c++\nclass Solution {\npublic:\n    vector<int> sortedSquares(vector<int>& nums) {\n        vector<int> ans(nums.size());\n        int FLAG=1;\n        int r=0;\n        while(nums[r]<0){\n            r++;\n            if(r>=nums.size())break;\n        }\n        int l=r-1;\n        if(l<0||r>=nums.size())FLAG=0;\n        int idx=0;\n        while(FLAG&&idx<nums.size()){\n            if(nums[l]*(-1)<nums[r]){\n                ans[idx]=nums[l]*nums[l];\n                l--;\n                if(l<0)FLAG=0;\n            }else{\n                ans[idx]=nums[r]*nums[r];\n                r++;\n                if(r>=nums.size())FLAG=0;\n            }\n            idx++;\n        }\n\n        if(FLAG==0){\n            if(l<0){//剩下右边\n                while(idx<nums.size()&&r<nums.size()){\n                    ans[idx]=nums[r]*nums[r];\n                    idx++;\n                    r++;\n                }\n            }else{\n                 while(idx<nums.size()&&l>=0){\n                    ans[idx]=nums[l]*nums[l];\n                    idx++;\n                    l--;\n                }\n            }\n        }\n\n        return ans;\n\n    }\n};\n\n```\n## <span id=\"head4\">java solution</span>\n\n```java\nclass Solution {\n    public int[] sortedSquares(int[] nums) {\n        \n        int len=nums.length;\n        int[]ans=new int[len];\n        int FLAG=1;\n\n        int r=0;\n        while(nums[r]<0){\n            r++;\n            if(r>=len){\n                FLAG=0;\n                break;\n            }\n        }\n\n        int l=r-1;\n        if(l<0) FLAG=0;\n\n        int idx=0;\n        while(FLAG==1&&idx<len){\n\n            if(nums[l]*(-1)<nums[r]){\n                ans[idx]=nums[l]*nums[l];\n                l--;\n                if(l<0)FLAG=0;\n            }else{\n                ans[idx]=nums[r]*nums[r];\n                r++;\n                if(r>=len)FLAG=0;\n            }\n\n            idx++;\n        }\n\n        if(FLAG==0){\n            if(l<0){//只剩右边\n\n                while(idx<len&&r<len){\n                    ans[idx]=nums[r]*nums[r];\n                    idx++;\n                    r++;\n                }\n                \n            }else{\n                while(idx<len&&l>=0){\n                    ans[idx]=nums[l]*nums[l];\n                    idx++;\n                    l--;\n                }\n            }\n        }\n        return ans;\n    }\n}\n```\n\n## <span id=\"head5\">improved solution</span>\n\n显然代码短了很多\n\n```c++\nclass Solution{\n\n public:\n\n    vector<int> sortedSquares(vector<int>& nums){\n        int len = nums.size();\n        vector<int>ans(len);\n\n        int l=0;\n        int r=len-1;\n        int idx=len-1;\n\n        while(idx>=0){\n            if(nums[l]*nums[l]>nums[r]*nums[r]){\n                ans[idx]=nums[l]*nums[l];\n                l++;\n            }else{\n                ans[idx]=nums[r]*nums[r];\n                r--;\n            }\n            idx--;\n        }\n\n        return ans;\n    }\n\n};\n\n```","tags":["blog"],"categories":["algorithm-training"]},{"title":"Diary 5/13","url":"/2023/05/13/5-13-随笔/","content":"# 5/13 \n\n怎么形容这一天？\n\n```mermaid\nflowchart TD\n    A[Christmas] -->|Get money| B(Go shopping)\n    B --> C{Let me think}\n    C -->|One| D[Laptop]\n    C -->|Two| E[iPhone]\n    C -->|Three| F[fa:fa-car Car]\n```\n\n","tags":["diary"],"categories":["diary"]},{"title":"Array 2:Remove Element","url":"/2023/05/11/5-11-array-2-remove-element/","content":"\n- [27. 移除元素](#head1)\n    - [C++ solution](#head2)\n    - [Java solution](#head3)\n- [26. 删除有序数组中的重复项](#head4)\n    - [C++ solution](#head5)\n    - [Java solution](#head6)\n\n\n\n# <span id=\"head1\">[27. 移除元素](https://leetcode.cn/problems/remove-element/)</span>\n\n![27](5-11-array-2-remove-element/1.png)\n\n## <span id=\"head2\">C++ solution</span>\n\n```c++\nclass Solution {\npublic:\n    int removeElement(vector<int>& nums, int val) {\n\n       int cur_index =0;\n       int length=nums.size();\n       int last_index=length-1;\n\n       if(length==1&&nums[0]==val)return 0;\n\n       while(cur_index<length){\n           if(nums[cur_index]==val){\n               while(nums[last_index]==val){\n                   length--;\n                   if(length==0)return 0;\n                    last_index=length-1;\n                    \n               }\n               if(cur_index<last_index){\n                    int t=nums[last_index];\n                    nums[last_index]=nums[cur_index];\n                    nums[cur_index]=t;\n               }\n               \n           }\n           cur_index++;\n       }\n         \n\t\treturn length;\n    }\n};\n```\n\n## <span id=\"head3\">Java solution</span>\n\n```java\n    class Solution {\n    public int removeElement(int[] nums, int val) {\n      \n      int cur=0;\n      int length=nums.length;\n      int last=length-1;\n\n      if(length==0)return 0;\n\n      while(cur<length){\n\n          if(nums[cur]==val){\n\n              while(nums[last]==val){\n                    length--;\n                    if(length==0)return 0;\n                    last=length-1;\n              }\n              if(cur<last){\n                  int t=nums[cur];\n                  nums[cur]=nums[last];\n                  nums[last]=t;\n              }\n          }\n          cur++;\n      }\n\n        return length;\n    }\n\n}\n```\n\n#  class Solution{​ public:    vector<int> sortedSquares(vector<int>& nums){        int len = nums.size();        vector<int>ans(len);​        int l=0;        int r=len-1;        int idx=len-1;​        while(idx>=0){            if(nums[l]*nums[l]>nums[r]*nums[r]){                ans[idx]=nums[l]*nums[l];                l++;            }else{                ans[idx]=nums[r]*nums[r];                r--;            }            idx--;        }​        return ans;    }​};​c++\n\n![26](5-11-array-2-remove-element/2.png)\n\n## <span id=\"head5\">C++ solution</span>\n\n```c++\nclass Solution {\npublic:\n    int removeDuplicates(vector<int>& nums) {\n        int cur=0;\n        int next=cur+1;\n\n        if(nums.size()==1)return 1;\n\n        while(cur<=nums.size()-1){\n\n            while(next<nums.size()&&nums[next]==nums[cur])next++;\n            cur++;\n            if(cur>=nums.size()||next>=nums.size())break;\n\n            nums[cur]=nums[next];\n            \n            \n        }\n        return cur;\n    }\n    \n};\n```\n\n## <span id=\"head6\">Java solution</span>\n\n```java\nclass Solution {\n    public int removeDuplicates(int[] nums) {\n        int cur=0;\n        int next=cur+1;\n\n        if(nums.length==1)return 1;\n\n        while(cur<=nums.length-1){\n           \n            while(next<nums.length&&nums[next]==nums[cur])next++;\n            cur++;\n            if(cur==nums.length||next==nums.length)break;\n            nums[cur]=nums[next];\n\n        }\n\n        return cur;\n    }\n}\n```","tags":["blog"],"categories":["algorithm-training"]},{"title":"Array 1:Binary Search","url":"/2023/05/10/5-10-array-1-binary-search/","content":"\n[toc]\n\n\n# <span id=\"head1\"> __[Ⅰ 704 二分查找](https://leetcode.cn/problems/binary-search/)__ </span>\n\n\n![704](5-10-array-1-binary-search/1.png)\n\n## <span id=\"head2\"> NOTE</span>\n\n\n### <span id=\"head3\">1. 每次写二分最容易纠结的part就是下面C++代码里面的A,B,C,D行</span>\n- A 处是用nums.size()-1还是nums.size()?\n- B 处是 <= 还是 < ?\n- C 处是 mid 还是 mid - 1 ?\n- D 处是 mid 还是 mid + 1 ?\n\n实际上这些不同的本质实际上是对于数组区间的划分法不同：\n\n|     | 左闭右闭区间                   | 左闭右开区间                 |\n|-----|--------------------------|------------------------|\n| A   | ```high=nums.size()-1``` | ```high=nums.size()``` |\n| B   | ``` while(low<=high)```  | ``` while(low<high)``` |\n| C   | ``` high=mid-1;```       | ``` high=mid;```       |\n| D   | ```low=mid+1;```         | ```low=mid+1;```       |\n\n### <span id=\"head4\">2. 二分法取中间值防溢出</span>\n\n```c++\n mid = (low+high) / 2\n```\nE 处的写法在两大数相加的时候容易发生溢出，可以改成下面的写法：\n```c++\n mid = low + (( high - low ) / 2)\n```\n\n## <span id=\"head5\">C++ solution</span>\n```c++\nclass Solution {\npublic \n    int search(vector<int>& nums, int target) {\n\n/*A*/   int low=0,high=nums.size()-1;\n\n/*B*/   while(low<=high){\n\n/*E*/        int mid=(low+high)/2;\n\n            if(target==nums[mid]){\n                return mid;\n            }else if(target<nums[mid]){\n/*C*/           high=mid-1;\n            }else{\n/*D*/           low=mid+1;\n            }\n\n        }\n\n        return -1;\n    }\n};\n```\n## <span id=\"head6\">Java solution</span>\n```java\nclass Solution {\n    public int search(int[] nums, int target) {\n\n        int low=0,high=nums.length-1;\n\n        while(low<=high){\n\n            int mid=(low+high)/2;\n\n            if(nums[mid]==target){\n                return mid;\n            }else if(nums[mid]>=target){\n                high=mid-1;\n            }else{\n                low=mid+1;\n            }\n\n        }\n\n        return -1;\n\n    }\n}\n```\n\n\n# <span id=\"head7\">__[Ⅱ  35. 搜索插入位置](https://leetcode.cn/problems/search-insert-position/)__</span>\n\n![35](5-10-array-1-binary-search/2.png)\n\n## <span id=\"head8\">C++ solution</span>\n```c++\nclass Solution {\npublic:\n    int searchInsert(vector<int>& nums, int target) {\n        int low =0,high=nums.size()-1,mid,ans=nums.size();\n\n        while(low<=high){\n\n            mid =(low+high)>>1;\n\t\t\t\n            if(target<=nums[mid]){//在左边 \n            \tans=mid;\n                high=mid-1;\n            }else{\n                low=mid+1;\n            }\n        }\n        return ans;\n\n    }\n};\n```\n## <span id=\"head9\">Java solution</span>\n```java\n\nclass Solution {\n    public int searchInsert(int[] nums, int target) {\n\n        int low = 0, high = nums.length-1, ans = nums.length;\n\n        while(low<=high){\n\n            int mid = ( low + high ) >> 1;\n\n            if(target <= nums[mid]){// 在左边\n\n                ans = mid;\n\n                high = mid-1;\n\n            } else{\n\n                low = mid +1;\n\n            }\n        }\n         return ans;   \n    }\n}\n\n```\n\n# <span id=\"head10\">[ __Ⅲ 34. 在排序数组中查找元素的第一个和最后一个位置__](https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array/)</span>\n\n![34](5-10-array-1-binary-search/3.png)\n\n## <span id=\"head11\"> NOTE</span>\n\n### <span id=\"head12\">1. 使用二分法在有序数组中查找某个数的左边界，即某个数第一次出现的位置</span>\n\n\n\n![image-20230610130542299](5-10-array-1-binary-search/image-20230610130542299.png)\n\n\n\n### <span id=\"head13\">2. 使用二分法在有序数组中查找某个数的右边界，即某个数最后一次出现的位置</span>\n\n与查找左边界不同的一个关键点有计算mid的时候需要l+r+1！\n否则会陷入死循环\n\n\n![image-20230610130632156](5-10-array-1-binary-searchimage-20230610130632156.png)\n\n\n## <span id=\"head14\">C++ solution</span>\n\n```c++\n\nclass Solution {\npublic:\n    vector<int> searchRange(vector<int>& nums, int target) {\n        \n        int left = binarySearch(nums,target,true);\n        int right =  binarySearch(nums,target,false)-1;\n        \n        if(left <= right && right < nums.size() && nums[left] == target && nums[right] == target){\n        \t\n        \treturn vector<int>{left,right};\n\t\t}else{\n\t\t\treturn vector<int>{-1,-1};\n\t\t}\n        \n    }\n    \n    int binarySearch(vector<int>nums,int target,bool isLeft){\n    \tint low =0,high=nums.size()-1,mid,ans=nums.size();\n    \t\n    \twhile(low<=high){\n    \t\t\n    \t\tmid=(low+high)>>1;\n    \t\t\n    \t\tif(target<nums[mid]||isLeft&&target<=nums[mid]){\n    \t\t\tans = mid;\n    \t\t\thigh = mid - 1;\n\t\t\t\t}else{\t\t\t\t\n\t\t\t\tlow = mid + 1;\n\t\t\t}\n\t\t}\n    \treturn ans;\n\t}\n    \n};\n\n```\n\n## <span id=\"head15\">Java solution</span>\n```java\nclass Solution {\n    public int[] searchRange(int[] nums, int target) {\n      int [] arr={-1,-1};\n\t\t\tint left = binarySearch(nums, target, true);\n\t\t\tint right = binarySearch(nums, target, false)-1;\n\n\t\t\tif(left<=right && right<nums.length && nums[left] == target && nums[right] == target){\n\t\t\t\tarr[0]=left;\n\t\t\t\tarr[1]=right;\n\t\t\t} \n\t\t\treturn arr;\n\n    }\n\t\tpublic int binarySearch(int[] nums, int target, boolean isLeft){\n\t\t\tint low = 0, high = nums.length-1, mid, ans = nums.length;\n\t\t\twhile(low<=high){\n\t\t\t\tmid=(low+high)>>1;\n\t\t\t\tif(target < nums[mid]|| isLeft && target <= nums[mid]){\n\t\t\t\t\tans = mid;\n\t\t\t\t\thigh = mid - 1;\n\t\t\t\t}else{\n\t\t\t\t\tlow = mid + 1;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n}\n```","tags":["blog"],"categories":["algorithm-training"]}]