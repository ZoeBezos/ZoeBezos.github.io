[{"title":"Software Development Ideas","url":"/2023/06/10/6-10-software-development-ideas/","content":"\n[toc]\n\n# 生态性质的“管家式”软件\n\n- 多方面的复盘\n  - 时间复盘\n    - 睡眠时间，起床时间，入睡时间\n    - 应用软件使用时间\n    - 学习、工作、健身、读书时间\n  - 金钱复盘\n\n- 多维度的复盘\n  - 按照日、周、月、季度、年复盘\n\n- 事务管理\n\n  | 睡眠 | 8h   |\n  | ---- | ---- |\n  | 学习 | 8h   |\n  | 健身 | 3h   |\n  | 读书 | 1h   |\n  | 其他 | 4h   |\n\n\n\n可以参考\n\n<img src=\"6-10-software-development-ideas/image-20230610225907531.png\" alt=\"image-20230610225907531\" style=\"zoom:33%;\" />\n\n<img src=\"6-10-software-development-ideas/image-20230610225938260.png\" alt=\"image-20230610225938260\" style=\"zoom:33%;\" />\n\n# .md文件转化为毕业论文\n\n## 用处：\n\n按照标题的级数自动转换格式\n\n## 人群\n\n写毕业论文的年轻人\n\n# 智能衣橱\n\n## 用处：\n\n​\t可以方便使用胶囊衣柜的人，每天穿干净和前天不同的衣服\n\n## 人群：\n\n​\t专攻性价比，瞄准刚毕业的年轻人\n","tags":["preview"],"categories":["interest-list"]},{"title":"Book List","url":"/2023/06/09/6-9-book-list/","content":"\n[toc]\n\n# 小说\n\n- [ ] 三体\n- [ ] 白鹿原\n- [ ] 使女的故事\n- [ ] 美丽新世界\n- [ ] 1984\n\n\n\n# 传记\n\n- [ ] 巴菲特传\n- [ ] 稻盛和夫自传\n- [ ] 钱钟书传\n- [ ] 成为波伏娃\n- [ ] 苏格拉底传\n\n# 逻辑类\n\n- [ ] 我为什么不结婚\n- [ ] 厌女\n- [ ] 君主论\n- [ ] \n\n# 医学健康类\n\n- [ ] 护照\n- [ ] 施瓦辛格健身全书\n- [ ] 皮肤的秘密\n\n# 人类/心理/社会类\n\n- [ ] 心理学与生活\n- [ ] 跨越边界的社区\n- [ ] 乌合之众\n- [ ] 人性心理学\n- [ ] 乡土中国\n\n# 法律类\n\n- [ ] 刑法学讲义\n- [ ] 洞学奇案\n\n# 财经类\n\n- [ ] 经济学原理\n- [ ] 货币金融学\n- [ ] 聪明的投资者\n\n# 计算机发展史\n\n- [ ] 浪潮之巅\n\n# 历史类\n\n- [ ] 明朝那些事\n- [ ] 中国通史\n- [ ] 全球通史\n- [ ] 极简宇宙史\n\n# 哲学\n\n- [ ] 大问题 简明哲学导论\n- [ ] 理想国\n- [ ] 沉思录\n- [ ] 哲学的历程\n- [ ] 认识你自己\n- [ ] 纯粹理性批判\n- [ ] 存在与虚无\n- [ ] 逻辑与哲学\n- [ ] 你的第一本哲学书\n\n# 艺术\n\n- [ ] 艺术与生活\n\n# 设计\n\n- [ ] 给大家看的设计书\n\n# 摄影\n\n- [ ] 美国纽约摄影学院摄影教材\n\n# 电影\n\n- [ ] 认识电影\n\n# 传播学\n\n- [ ] 舆论\n\n# 考古\n\n- [ ] 考古学与史前文明\n\n# 生物学\n\n- [ ] 所罗门王的指环\n\n# 死亡\n\n- [ ] 安乐死现场\n\n# 俄罗斯文学\n\n- [ ] 白夜\n- [ ] 罪与罚\n- [ ] 白痴\n- [ ] 卡拉马佐夫兄弟\n- [ ] 少年\n- [ ] 群魔\n- [ ] 地下室手记\n\n# 人生意义\n\n- [ ] 编周记\n\n# 两性婚姻\n\n- [ ] 爱的艺术\n\n# 情绪管理\n\n- [ ] 伯恩斯新情绪疗法\n\n# 自然\n\n- [ ] 看不见的自然\n\n# 宗教\n\n- [ ] 佛教的见地和修道\n\n","tags":["hobby"],"categories":["reading"]},{"title":"Graduate Paper","url":"/2023/06/01/6-1-my-paper/","content":"\n[toc]\n\n![img](6-1-my-paper/wps1.jpg) \n\n# 毕业设计说明书\n\n \n\n \n\n|                            作  者                            |             山杜哈西·土鲁四拜克              |\n| :----------------------------------------------------------: | :------------------------------------------: |\n|                            学 号                             |                 919106840208                 |\n|                             学院                             |             计算机科学与工程学院             |\n|                          专业(方向)                          |                智能科学与技术                |\n|                            题  目                            | 基于图神经网络和注意力机制的社交推荐算法研究 |\n| class Solution{​ public:    vector<int> sortedSquares(vector<int>& nums){        int len = nums.size();        vector<int>ans(len);​        int l=0;        int r=len-1;        int idx=len-1;​        while(idx>=0){            if(nums[l]*nums[l]>nums[r]*nums[r]){                ans[idx]=nums[l]*nums[l];                l++;            }else{                ans[idx]=nums[r]*nums[r];                r--;            }            idx--;        }​        return ans;    }​};​c++ |                孟顺梅 副教授                 |\n|                            评阅者                            |                杜鹏桢 副教授                 |\n\n \n\n \n\n2023  年  5  月\n\n \n\n \n\n\n\n \n\n \n\n# 声\t明\n\n \n\n我声明，本毕业设计说明书及其研究工作和所取得的成果是本人在导师的指导下独立完成的。研究过程中利用的所有资料均已在参考文献中列出，其他人员或机构对本毕业设计工作做出的贡献也已在致谢部分说明。\n\n本毕业设计说明书不涉及任何秘密，南京理工大学有权保存其电子和纸质文档，可以借阅或网上公布其部分或全部内容，可以向有关部门或机构送交并授权保存、借阅或网上公布其部分或全部内容。\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n\n# 中文摘要\n\n 摘要：\n\n社交网络的日益普及，促进了社交推荐领域的迅猛发展。随着数据规模的不断增大，社交推荐问题变得越来越复杂和具有挑战性。为了解决传统推荐算法存在的问题，本文提出了一种基于图神经网络和注意力机制的社交推荐算法。该算法通过将用户行为和社交网络信息融合到一个图中，然后使用图神经网络对图进行学习，获取每个用户和项目的嵌入向量。同时，引入注意力机制，以更好地捕捉用户在社交网络中的交互和兴趣相关性。最后，利用学习到的嵌入向量和交互行为信息进行推荐。实验结果表明，该算法 在三个真实数据集上的表现比现有的深度学习推荐系统都有较大提升，证明了该算法在社交推荐中的有效性。   \n\n关键词：\n\n图神经网络 社交推荐算法 注意力机制 用户兴趣建模 \n\n \n\n\n\n# 毕业设计说明书外文摘要\n\n **Title:**\n\nResearch on Social Recommendation Algorithm Based on Graph Neural Network and Attention Mechanism  \n\n**Abstract:**\n\nThe increasing popularity of social networks has promoted the rapid development of the field of social recommendation. With the continuous increase of data scale, the social recommendation problem becomes more and more complex and challenging. In order to solve the problems existing in traditional recommendation algorithms, this paper proposes a social recommendation algorithm based on graph neural network and attention mechanism. The algorithm combines user behavior and social network information into a graph, and then uses a graph neural network to learn the graph to obtain the embedding vector of each user and item. At the same time, an attention mechanism is introduced to better capture user interactions and interest correlations in social networks. Finally, recommendations are made using the learned embedding vectors and interaction behavior information. The experimental results show that the performance of the algorithm on two real data sets is greatly improved compared with the traditional matrix factorization and deep learning methods, which proves the effectiveness of the algorithm in social recommendation. \n\n **Keywords：**\n\ngraph neural network, social recommendation algorithm, attention mechanism, user interest modeling\n\n \n\n\n\n \n\n\n\n# 1 引言\n\n近年来，社交推荐算法作为社交媒体中的重要应用之一，受到越来越多的关注。然而，传统的协同过滤和内容推荐算法在社交推荐中存在着数据稀缺、冷启动问题、长尾效应等诸多局限和挑战。因此，如何利用图神经网络和注意力机制来解决这些问题成为当前社交推荐算法研究的热点。基于图神经网络和注意力机制的社交推荐算法可以充分提取社交网络的结构特征和情感语义信息，从而更准确地推荐用户感兴趣的内容，提高质量和效率。本文旨在对此类算法进行综述和探索，以期为社交推荐算法的开发和应用提供一些参考。\n\n## 1.1 研究背景及其意义\n\n随着网络技术的飞速发展和社交网络的普及，社交网络已经成为人们日常生活中不可缺少的一部分。社交网络的用户数量正在快速增长，积累了庞大的用户行为数据，成为进行社交推荐的宝贵资源。所谓社交推荐，是指在社交网络中，利用用户的社交关系、兴趣、行为等信息，向用户介绍个性化、多样化的内容和服务。在社交推荐中，传统的推荐算法，如基于内容的推荐、协作过滤、隐语义模型等，从单一角度对用户进行建模推荐只能进行修改，很难从多个角度理解用户的兴趣和行为，导致推荐的准确度和多样性不足。\n\n近年来，深度学习技术的兴起为解决传统推荐算法的局限性提供了有效的解决方案。深度学习是一种具有多层神经网络的机器学习方法，其可以自动地从数据中学习多层抽象特征，表达数据的内在结构并对新的数据进行推理和预测。其中，基于图神经网络(Graph Neural Network,GNN)和注意力机制(Attention Mechanism)的社交推荐算法，则成为了当前研究的热点之一。\n\nxxxxxxxxxx class Solution {    public int[] searchRange(int[] nums, int target) {      int [] arr={-1,-1};            int left = binarySearch(nums, target, true);            int right = binarySearch(nums, target, false)-1;​            if(left<=right && right<nums.length && nums[left] == target && nums[right] == target){                arr[0]=left;                arr[1]=right;            }             return arr;​    }        public int binarySearch(int[] nums, int target, boolean isLeft){            int low = 0, high = nums.length-1, mid, ans = nums.length;            while(low<=high){                mid=(low+high)>>1;                if(target < nums[mid]|| isLeft && target <= nums[mid]){                    ans = mid;                    high = mid - 1;                }else{                    low = mid + 1;                }                            }            return ans;        }}java\n\n除了基于图神经网络的算法，注意力机制也被广泛应用于社交推荐算法中。注意力机制是一种重要的机器学习技术，通过给不同节点或边赋予不同的权重来调节模型的关注度，从而使模型更关注于对当前任务最有用的信息。在社交推荐中引入注意力机制，可以使得模型更加关注用户和物品之间最为重要的关系和信息，从而提高推荐的精度和效率。事实上，注意力机制已经成为当前最先进的深度学习技术之一，其应用及变种层出不穷。通过将注意力机制应用于社交推荐中，不仅可以提高推荐效果，还可以在复杂的网络结构中发现隐藏的信息和规律，这有助于发现社交网络中影响用户兴趣和行为的关键因素，为执行更加有效的社交推荐提供支持。\n\n因此，研究基于图神经网络和注意力机制的社交推荐算法不仅有助于解决社交推荐领域的问题，还可以推动其他领域的发展。这种研究具有重要的理论和实际意义。通过研究基于图神经网络和注意力机制的社交推荐算法，可以提高推荐服务的精度和多样性，更好地满足用户需求。同时，该研究也有助于推动深度学习技术的发展和创新，为未来的研究提供有价值的参考。\n\n## 1.2 国内外研究现状\n\n基于图神经网络和注意力机制的社交推荐算法是近年来热门的研究方向之一。国内外许多研究者针对这一算法进行了深入的探讨和实验研究。本小节将对其国内外的研究现状进行详细介绍。\n\n### 1.2.1 基于图神经网络的推荐系统研究现状\n\n近年来，图形表示作为一种强大的建模方法得到了广泛的研究，并应用在各个领域：社交媒体中的友谊分析[ 1]、生物学中的蛋白质相互作用建模 [2]、社会学中构建推理知识图等 [3]。\n\n在推荐系统领域中，基于图的数据建模可以通过扩展相似性来覆盖实体之间的关系，从而帮助克服稀疏性等问题[ 4]。图建模的推荐系统中定义相似性有多种方法，包括元路径分析 [5]、 模拟随机游走[ 6]和网页排序算法 [7]。然而，这些方法的主要问题是，不清楚哪条路径可以反应图中实体之间的相似性[ 4]。随着学者们进一步研究发现，该相似性的计算可以被用户和 项目节点的嵌入所取代。而图神经网络就是一种通用的方法，用于提取图中节点的嵌入。 \n\n图神经网络（GNN）是一种从卷积神经网络（Convolutional  Neural Net-works，CNN[38]）启发而来的新型神经网络，它可以从节点信息和图的拓扑结构中学习[ 8]，并且有助于合并用户和项目之间的多跳连接信息 。图神经网络通常使用聚合模块和更新模块来嵌入节点。图神经网络可以决定将多少信息从一个节点传播到它的邻居节点，以及如何聚合从邻居节点接收到的所有信息，同时考虑节点之间相应边的权重[ 9]。其中聚合模块和更新模块可以使用不同的方法：在聚合模块中，Ying等人[ 10]使用最大池化和平 均池化进行聚合邻居节点信息，而Hamilton等人[ 2]使用了LSTM聚合器。在更新模块中，He 等人[ 11]将聚合信息作为目标节点的新表示；Hamilton等人 [2]将聚合信息与目标节点的原始信 息进行聚合获得最终嵌入。Monti等人[ 12]在提取嵌入信息的过程中，将附加信息与用户反馈结合在一起，并且修改了ChebNet[ 13]中的卷积运算，从用户图和项目图中分别提取用户和项目嵌入，通过使用循环神经网络（Recurrent Neural Network，RNN）的扩散过程逐步改善提 取的嵌入，用来正确预测未知反馈。Berg 等人[ 14]提出的GC-MC模型是第一个在推荐系统中使用图自动编码器 （Graph Auto Encoder，GAE）的模型，与专注于节点嵌入的模型不同， 该模型直接通过单个非迭代图神经网络提供预测，并且使用了图卷积编码器和解码器来预测用户项目节点之间边的权重。Fan等人[ 15]提出的GraphRec模型中，用户信息由社交网络中的邻居用户和该用户本身两部分组成，使用平均池化来聚合邻居的信息。利用多层感知机对未知反馈进行预测，该多层感知机基于邻居用户获得目标项目表示，基于其用户和邻居获得目标用户表示。Wu等人[16]是第一个提出基于会话的图神经网络推荐系统。 \n\n图卷积神经网络（Graph Convolutional Network，GCN）是最常用的图神经网络方法， 它根据节点之间的权重组合节点的特征向量，对节点进行编码。用双线性解码器获取用户和 项目的编码向量，并重建用户反馈。主要分为两大类：基于谱（spectral-based）和基于空间 （spatial-based）。谱图卷积神经网络表示基于拉普拉斯矩阵的图，计算拉普拉斯矩阵需要大 量的计算时间。并且也不可能将经过训练的模型从特定结构转移到另一个图形。另一方面， 空间图卷积神经网络在处理不同大小的邻居节点和节点权重矩阵方面也存在一些挑战。\n\nPinSage模型是Ying等人[ 10]通过图卷积神经网络开发的，也是图卷积神经网络模型第一次成功应用于商业推荐系统。PinSage没有使用k-hop算法，而是基于随机游走算法对相似的无向 邻居节点进行采样，然后将生成的嵌入与节点的初始嵌入连接起来，并通过另一个密集层来 获得最终表示。Zhang等人[ 16]提出了IG-MC模型，该模型中对于每个用户的交互项目，提取 一个包含用户和项目的封闭子图，与PinSage不同，后者通过提取子图的嵌入，对该子图节 点之间的相关性进行建模，然后聚集节点来表示子图。Sun等人[ 17]提出的Multi-GCCF模型 中，把图卷积神经网络应用于用户图的邻域信息中，将节点信息与一阶邻域用户节点信息合 并嵌入。在NGCF模型[ 18]中，用户和项目的高阶连通性被考虑在用户项目二分图中，并且在聚合了高阶邻居信息的同时，该模型还通过聚合不同阶的嵌入来表示节点的最终嵌入。为了收集用户兴趣点的偏好数据，Zhong等人[ 19]在图卷积神经网络上融合了多头注意机制。\n\nHekmatfar 等人[ 20]提出的PGRec模型可以预测用户和偏好节点之间的连接权重。该图采用了单层谱图卷积神经网络，可以基于目标节点和每个邻域之间的路径数量，计算每个间接邻域的权重，从而确定目标节点中每个邻域的影响量。Duran等人[ 21]提出的基于会话推荐的增强 图神经网络模型可以充分利用上下文信息，并在用户上下文项目图上应用图卷积神经网络。 其中用户节点是基于该节点交互项目的传播信息，以及用户和项目之间交互的上下文来表示的。Song等人[ 22]提出的GACSE模型考虑了静态嵌入（即所有节点的归一化平均值）和动态嵌入，并且通过图注意力机制计算邻居的加权平均值。他们还提出将贝叶斯个性化排序和节 点相似性相结合，作为一个面向排序的损失函数。 \n\n图注意力网络（Graph Attention Networks，GAT）是一种空间图卷积神经网络方法，在 图神经网络的传播步骤中采用注意力机制[ 23]。它根据每个邻居节点与目标节点的相似性计算 每个邻居节点的注意力系数，该相似性是从其相应的特征向量中推导出来的[ 24]。注意力机制以多种方式应用于图注意力网络，Chen等人[ 25]为不同特性的邻居节点分配不同的注意力系数。在异构图中，注意力机制主要用于为每种类型的节点[ 26]和边 [27]或不同的元路径 [5]分配 不同的系数。在频谱图神经网络中，图注意力网络从低频（与小特征值相关的特征向量）到 高频（与大特征值相关的特征向量）提取不同频率分量的重要性[ 28]。为了评估每个用户的社会影响，Wu等人[ 29]首次在推荐系统上应用了图注意力网络。该模型考虑了社交网络中每个 邻居的排他性权重。\n\n### 1.2.2 融合注意力机制的推荐系统研究现状\n\n随着信息技术的快速发展和网络交流的普及，线上交流和沟通已经成为人们日常生活不可或缺的一部分，这也为推荐系统带来了新的发展机遇。 研究者们开始探索使用注意力机制来优化[ 30]推荐系统并取得了显著的进展，通过大量实验[ 31]验证，证明了注意力机制可以有效提高推荐系统的准确性。\n\n根据Pei等人的观点[ 32]，现有的推荐系统中通常将用户项目的全部历史信息视为同等相关，但这种处理方式并不适用于现实场景。 柴玉梅等人[33]发现，在评论文本驱动的推荐系统中，很多方法只考虑用户对项目的评分和评论，却没能充分挖掘评论文本中蕴含的有价值的信息。为此，研究者们提出了一种双重注意力机制来增强对文本信息的关注并获取更多有意义的信息，从而改善推荐系统的性能。Chen等人[34]在因子分解机器中应用神经注意力机制网络来学习用户与项目之间的交互特征，并将注意力网络组件进行聚合。将注意力机制和项目结合后，研究者们成功地获取了用户的最终表示，并将其用于推荐任务。研究者Guo等人[35]提出了一种神经注意力推荐模型，利用用户的隐式反馈行为，将用户转化为向量形式，然后从用户的项目历史中提取时序信息。\n\n## 1.3 总体技术方案及其社会影响\n\n本课题要解决的问题是基于图神经网络和注意力机制的社交推荐算法研究，解决思路步骤是研究并掌握现有技术、构建模型、选取合适数据集、训练模型、比较结果验证其有效性。研究并掌握多种融合注意力机制和图神经网络的社交推荐模型（如 DiffNet++[40]，ASR，LightGCN[11] 等），研究图经网络技术，与现有推荐模型相结合。通过对用户的历史交互行为分别分析和构建用户偏好与项目相关的模型，融合注意力机制，利用获取的用户及项目特征进行推荐，基于图神经网络和注意力机制的社交推荐算法。最后选择了LastFM、Yelp 和Douban-book数据集，对提出算法进行实现，并与现有推荐技术（DiffNet[39]、DiffNet++[40]和LightGCN[11] ）进行比较，通过 Recall 和NDCG两个评价指标来论证模型的有效性。\n\n该算法的社会影响主要有两个方面。首先，推荐系统对于提高信息的个性化呈现、提高用户体验、提高品牌商家曝光度和提高销售业绩等方面起着重要作用。其次，该算法使用到了图神经网络和注意力机制等前沿技术，为相关领域提供了研究参考。\n\n## 1.4 技术方案的经济因素分析\n\n本课题充分考虑了技术方案的经济可行性，选取了合适的数据集便于普通的个人电脑GPU训练模型，无需额外购买设备，可行性高。\n\n## 1.5 论文章节安排\n\n本文主要分为五个章节，详细安排如下所示：\n\n第一章节，绪论部分，简要介绍了课题的研究背景及其意义，详细描述了国内外研究现状和重点，并介绍了本文主要工作。\n\n第二章节，理论部分，介绍了推荐系统、图神经网络、注意力机制等相关的基础理论知识。\n\n第三章节，模型部分，提出了基于图神经网络和注意力机制的社交推荐算法研究，模型结合图神经网络和注意力机制基于用户之间社交关系进行推荐。\n\n第四章节，实验部分，将模型应用于多个真实数据集，并探究了模型各参数对推荐性能的影响程度，与当下几个主流推荐算法进行对比分析，验证了模型的有效性。\n\n第五章节，总结部分，梳理了本课题成果和改进之处，并对下一阶段学术研究工作进行展望。\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n# 2  相关技术理论\n\n本章将详细阐明推荐系统技术、图神经网络模型以及注意力机制相关的基础理论知识。\n\n## 2.1 推荐算法\n\n推荐算法是计算机科学和人工智能领域的一个重要分支，也是互联网和移动互联网应用中必不可少的一部分。推荐算法通过对用户历史行为、个人偏好、社交关系等信息的分析，为用户推荐个性化的内容，提高了用户体验和满意度，并对商业价值的实现起到了重要的促进作用。\n\n推荐算法的关键在于利用机器学习、数据挖掘等方法来分析用户的行为和兴趣，以便向用户推荐最符合他们需求的内容。一般而言，推荐算法主要有三种基本方法，即基于内容的推荐、协同过滤推荐和基于规则的推荐。\n\n基于内容的推荐方法是根据物品的属性信息（如电影的演员、导演等）计算物品之间的相似度，从而为用户推荐与其已经喜欢的物品相似的物品。该方法的优点是可以推荐新物品，但是缺点是很难捕捉用户的兴趣演变趋势。\n\n协同过滤推荐方法通过用户行为数据（如用户购买、评价、浏览等记录）寻找用户和物品之间的关联关系，从而推荐用户可能感兴趣的物品。该方法的优点是可以适应用户兴趣的演变和变化，但是缺点是很难处理稀疏数据。\n\n基于规则的推荐方法则是根据预先设定的规则筛选出用户可能感兴趣的物品。该方法的优点是不需要考虑用户行为数据，但是缺点是需要手动设定规则，且规则使用效果容易受到规则设置的限制。\n\n近年来，随着互联网应用的不断发展和数据量的增加，人工智能技术的进步也带来了推荐算法的新变革，如基于深度学习的推荐算法、基于知识图谱的推荐算法等。\n\n基于深度学习的推荐算法通过深度神经网络学习用户和物品之间的交互特征，进而预测用户可能感兴趣的物品，并实现个性化推荐。该方法可以从多个角度和维度对用户进行建模和理解，进一步提高了个性化推荐的准确性和独特性。\n\n基于知识图谱的推荐算法则是通过将用户行为数据映射到结构化的图形知识库上，进而针对用户和物品进行推荐。该方法可以使用多种关系和约束来描述用户和物品之间的关系，进一步提高了推荐的准确性。\n\n### 2.1.1  基于内容的推荐算法 \n\n​    基于内容的推荐算法是一种常用推荐算法，它可以根据物品之间的相似性计算得分，然后推荐与用户过去喜好相似的物品（如图2-1）。该算法主要利用物品的内容信息来计算相似度，并且在推荐系统的音乐、电影、新闻等领域得到广泛应用。\n\n![img](6-1-my-paper/wps2.jpg) \n\n图 2-1 基于内容的推荐算法\n\n基于内容的推荐算法实现流程包括以下几个步骤：首先，从物品的描述信息中抽取出所需的特征，如标签、分类、关键词等，并将它们转化为机器可理解的表示方式，例如向量或矩阵。然后，对于这些表示方式，进行常规的相似度计算方法，例如余弦相似度或欧几里德距离等，来计算物品之间的相似度。最后，根据计算出的物品相似度进行相似物品推荐，选择与目标物品相似度最高的物品，将它作为推荐结果返回给用户。\n\n### 2.1.2 协同过滤算法\n\n协同过滤算法是一种广泛应用的推荐算法，它综合了基于内容的推荐算法的优点，并利用用户历史行为数据，预测目标用户的兴趣偏好，从而实现了更加准确的推荐。协同过滤算法主要有两种模型：基于用户的协同过滤和基于物品的协同过滤。基于用户的协同过滤算法通过分析用户之间的相似性，将目标用户的兴趣偏好与相似用户的偏好进行比较，来进行推荐。而基于物品的协同过滤算法则通过分析物品之间的相似性，将目标用户曾经喜欢的物品与相似的物品进行比较，从而进行推荐。\n\n(1) 基于用户的协同过滤算法\n\n基于用户的协同过滤算法通过分析多个用户的历史行为数据，找到与目标用户具有相似兴趣偏好的其他用户，并向目标用户推荐这些用户喜欢的物品。具体的实现流程如下：首先，收集目标用户的历史行为数据，例如浏览、点击、购买等信息。接着，通过相似度计算方法，例如余弦相似度、欧几里德距离等，计算目标用户和其他用户之间的相似度，并选择与目标用户相似度最高的K个用户。最后，从这K个用户中挑选出他们喜欢的物品，并将这些物品推荐给目标用户（如图2-2）。\n\n![img](6-1-my-paper/wps3.jpg) \n\n图 2-2  基于用户的协同过滤推荐系统\n\n(2) 基于物品的协同过滤算法\n\n基于物品的协同过滤算法利用物品之间的相似性关系，找到与目标物品相似的其他物品，并向用户推荐这些相似的物品。具体的实现流程如下：首先，收集目标用户的历史行为数据，例如浏览、点击、购买等信息。然后，分析历史行为数据，找出目标用户对哪些物品感兴趣，以及哪些物品与目标物品相似。接着，基于物品相似度，从相似的物品中选取一定数量的物品作为推荐结果进行推荐给用户（如图2-3）。\n\n![img](6-1-my-paper/wps4.jpg) \n\n图 2-3 基于物品的协同推荐系统\n\n### 2.1.3 混合推荐算法\n\n混合推荐算法指的是将多种推荐算法结合起来使用，以提高推荐质量和准确性的一种方法。常见的组合方式有以下几种：\n\n(1)基于规则的组合：根据一定的规则，如用户的历史行为、商品的属性等，将多种算法的结果进行组合并加权，然后进行推荐。\n\n(2)基于特征的组合：将用户和商品的特征进行学习，并结合多种推荐算法进行预测和推荐。\n\n(3)基于模型的组合：将多种算法分别训练出来的模型进行融合，以得到更准确的推荐结果。\n\n(4)基于内容的组合：将基于内容的推荐算法与其他推荐算法结合使用，使用内容信息补充推荐模型缺失的信息，以提高推荐效果。\n\n![img](6-1-my-paper/wps5.jpg) \n\n图 2-4 不同情形之下可行推荐策略\n\n混合推荐算法具有较高的灵活性和适应性，并能够在不同的场景中得到应用。但是，也存在难以选择合适的组合方式和模型参数等问题。通常当用户请求推荐服务时，推荐接口服务先获取用户的协同过滤推荐结果，如果没有，再去取用户的基于内容的推荐结果，如果还没有，最后就用户热门默认推荐（如图2-4）。\n\n## 2.2图神经网络\n\n图神经网络(GNN)是一种用于处理图数据的神经网络。它的出现是为了解决传统神经网络不能处理非欧几里得结构数据的问题。图神经网络主要是用于解决节点分类、边分类、图分类等问题。它是由一系列基本的图神经网络模块组成的，包括图卷积神经网络(GCN)、图注意力网络(Graph Attention Network, GAT)以及基于图卷积拉普拉斯算子的谱卷积神经网络(Spectral Convolutional Neural Network, SCNN)等。\n\n图神经网络能够很好地捕捉图中节点和边之间的关系，具有良好的可解释性和泛化能力。因此，在社交网络分析、化学分子分析、图像分割等领域得到了广泛的应用。\n\n### 2.2.1 图卷积神经网络\n\n图卷积神经网络( GCN)是一种基于卷积操作的图神经网络，它可以用于学习节点在具有图结构的数据集上的表示，其网络结构如图2-5。它的基本思想是将节点的邻居节点信息进行卷积操作，获得每个节点的新特征表示，从而实现节点分类、链接预测等任务。\n\n![img](6-1-my-paper/wps6.jpg) \n\n图 2-5 图卷积网络模型\n\n\n\n对于一个图G，其中有M个节点，每个节点都有自己的特征，设这些节点的特征组成一个M×D维的矩阵A，然后各个节点之间的关系也会形成一个N×N维的矩阵X，也称为邻接矩阵。X和A便是模型的输入。GCN也是一个神经网络层，它的层与层之间的传播方式是：\n\n|      | ![img](6-1-my-paper/wps7.jpg) | (2-1) |\n| ---- | :---------------------------: | ----: |\n\n上式中，σ(·)是sigmoid函数，用于非线性激活；![img](6-1-my-paper/wps8.jpg)=![img](6-1-my-paper/wps9.jpg)+I，I是单位矩阵；![img](6-1-my-paper/wps10.jpg)是![img](6-1-my-paper/wps11.jpg)的度矩阵；H是每一层的特征，对于输入层，H就是A。\n\nGCN的学习过程需要通过多层卷积操作来逐步增强每个节点的邻居节点信息。在每一层中，GCN会先对节点及其邻居节点构成的局部子图进行卷积，然后使用池化或者其他操作将图缩小以进一步改善模型的效率。这种多层卷积的过程类似于深度神经网络中的多层感知机，因此，GCN也被称为图卷积多层感知机(Graph Convolutional Multi-Layer Perceptron, GC-MPL)。\n\nGCN被广泛应用于社交网络分析、图像数据处理、化学分子分析等领域，在不同的任务上都取得了良好的效果。\n\n### 2.2.2 图注意力网络\n\n图注意力网络(GAT)是一种基于注意力机制的图神经网络，它能够对每个节点的邻居节点信息进行加权聚合，从而捕捉节点和邻居之间更为精细、动态的关系。相对于传统的图卷积神经网络(GCN)，GAT具有更强的自适应能力和灵活性。\n\n在GAT中，每个节点的邻居节点信息会被映射到一个低维空间中，然后使用注意力权重来调节每个邻居节点的重要性，这些邻居节点的特征向量经过加权求和后，与该节点本身的特征向量一同输入到后续的神经网络层。这种注意力机制使得GAT能够自适应地对邻居节点进行加权，获得更为准确的节点表示。\n\nGAT 中的注意力机制可以通过学习得到，也可以事先设定为固定的形式。对于不同任务和数据集，我们可以选择不同的注意力机制来获得最佳的性能。GAT已经被证明在很多领域取得了优越的表现，包括节点分类、链接预测、社区发现等任务。\n\n## 2.3注意力机制\n\n注意力机制是一种人工智能算法，其基本思想是在模型中使用一个可学习的权重分配机制，使其能够将注意力集中在输入序列的某些部分上，以便提高模型的精度和准确性。\n\n![img](6-1-my-paper/wps12.jpg) \n\n图 2- 6  注意力机制模型\n\n注意力机制可抽象为一种通用的不依赖于具体框架的思想，其核心目标是从众多信息中选择出对当前任务目标更关键的信息。Encoder-Decoder框架的注意力模型，编码器使用RNN、CNN[38]等神经网络提取输入序列的特征，计算得到编码器的隐状态hi，而注意力模型中的上下文向量已不再是共享同一个上下文向量c，而是不同的时间步t使用不同的上下文向量ct。注意力机制模型如图2-6。由图可知，上下文向量ct可以关注到输入中最相关的部分，也就是“注意力”的计算式如下：\n\n|      | ![img](6-1-my-paper/wps13.jpg) | (2-1) |\n| ---- | :----------------------------: | ----: |\n\n如2-4式所示，在具体实现时，注意力机制常常采用“加权和”方式对输入序列进行加权计算。这些权重由激活函数产生，取决于输入序列中的每个元素和当前输出单元之间的相似程度。在每个时间步骤中，模型会基于一组权重对输入序列进行加权求和，以计算输出序列中的当前元素。\n\n \n\n \n\n \n\n\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n# 3 基于图神经网络和注意力机制的社交推荐算法研究\n\n现有的深度神经网络模型的推荐系统主要有基于社交网络图的DiffNet[39]、DiffNet++[40]和基于图形嵌入的轻量级推荐算法模型的方法 LightGCN[11] 。其中其中较为轻量的模型 （如LightGCN[11] ）未考虑用户之间关系网络导致数据稀缺；一些基于社交关系网络图的模型（如DiffNet[39]）未将注意力引入模型，未能充分挖掘数据的信息；一些在社交关系网络中引入了注意力机制的模型（如DiffNet++[40]）虽然考虑了用户社交关系网和图卷积网络并将注意力机制引入模型，但是存在参数冗余的问题，导致在复杂的社交关系网络中计算非常耗时费力。\n\n针对以上方法的缺陷，本文提出了一种基于图神经网络和注意力机制的社交推荐算法研究，本算法结合了图卷积神经网络、注意力机制等技术，并且利用利用社交网络数据来缓解数据稀疏性，通过上下文交互信息获取用户和项目当前相关信息的嵌入表示；然后在社交关系图和用户交互图卷积；接着融合注意力机制添加权重来区分不同尾实体在语境下的语义信息，最后在此基础上将用户和项目的表示向量聚合内积得到预测评分，并使用设置损失函数对模型进行学习。 \n\n具体的推荐模型结构如图3-1所示，模型框架可分为四部分：1）嵌入表示层；2）基于用户交互与社交网络的图卷积网络层；3）基于注意力机制的图聚合学习层；4）预测推荐层。下面将把模型这四部分结构详细阐明。\n\n \n\n![img](6-1-my-paper/wps14.jpg) \n\n图 3-1  模型结构图\n\n## 3.1 嵌入表示层\n\n模型根据主流推荐模型，用嵌入向量eu∈Rd描述一个用户u，用ei∈Rd描述一个项目i，其中d表示嵌入大小，如图3-2。这可以看作是构建一个参数矩阵作为一个嵌入的查找表E：\n\n|      | ![img](6-1-my-paper/wps15.jpg) | *3-1) |\n| ---- | :----------------------------: | ----: |\n\n![img](6-1-my-paper/wps16.jpg) \n\n图 3-2  用户和项目的嵌入表示\n\n \n\n## 3.2 基于用户交互与社交网络的图卷积网络\n\n### 3.2.1 用户-用户社交图卷积网络\n\n每个用户都对应一个固定的嵌入![img](6-1-my-paper/wps17.jpg)（即一组神经网络的权重)，表示该用户在整个社交网络中的潜在偏好。这些嵌入将作为网络的输入，用以引导社交网络中的信息扩散，模型如图3-3。\n\n![img](6-1-my-paper/wps18.jpg) \n\n图 3-3  用户社交图卷积模型\n\n \n\n该模型的输入是一个动态社会网络S，其中包含多个用户，即用户u有社交关系的用户集为![img](6-1-my-paper/wps19.jpg)。第n层将用户来自前一层的嵌入作为输入，并在当前的社会扩散过程完成后输出用户更新后的嵌入，然后更新后的用户嵌入被发送到第n+1层进行下一个扩散过程。\n\n|      | ![img](6-1-my-paper/wps20.jpg) | (3-2) |\n| ---- | :----------------------------: | ----: |\n\n其中Pool函数可以定义为一个平均池，它对所有邻居用户在第n层的潜在嵌入执行一个平均操作。该函数也可以定义为一个最大操作，选择所有邻居用户在第k层的潜在嵌入的最大元素，形成![img](6-1-my-paper/wps21.jpg)，卷积网络细节如图3-4。\n\n![img](6-1-my-paper/wps22.jpg) \n\n图 3-4 用户社交图卷积层细节\n\n### 3.2.2 用户-项目交互图卷积网络\n\n在用户嵌入和项目嵌入组成的二部图中，设置项目i有交互过的用户集为![img](6-1-my-paper/wps23.jpg)。同理，用户u有交互过的项目集为![img](6-1-my-paper/wps24.jpg)。每层图卷积网络的输入为用户嵌入eu（或项目嵌入ei）,按照其有交互的项目（用户）的数目，计算出更新过后的用户嵌入eu’(或项目嵌入ei’）并输出，卷积网络模型如图3-5。\n\n![img](6-1-my-paper/wps25.jpg) \n\n图 3-5 用户-项目交互图卷积模型\n\n为了采用简单加权和聚合器，不再使用特征变换和非线性激活。本模型中的图卷积运算定义为：\n\n|      | ![img](6-1-my-paper/wps26.jpg) | (3-3) |\n| ---- | :----------------------------: | ----: |\n|      | ![img](6-1-my-paper/wps27.jpg) | (3-4) |\n\n其中对称归一化项![img](6-1-my-paper/wps28.jpg)遵循标准GCN 的设计，它避免了嵌入规模随图卷积运算的增加，卷积网络细节如图3-6。\n\n![img](6-1-my-paper/wps29.jpg) \n\n图 3-6 用户-项目交互图卷积层细节\n\n \n\n## 3.3 基于注意力机制的图聚合学习\n\n为了实现自动优化注意力权重，模型设计了一个图形注意网络来自动学习图形集成的参数，如图3-7。\n\n![img](6-1-my-paper/wps30.jpg) \n\n图 3-7 本模型的注意力机制\n\n模型建模的图的注意力权重为![img](6-1-my-paper/wps31.jpg)和![img](6-1-my-paper/wps32.jpg)（d’是注意力的大小）为：\n\n|      | ![img](6-1-my-paper/wps33.jpg) | (3-5) |\n| ---- | :----------------------------: | ----: |\n|      | ![img](6-1-my-paper/wps34.jpg) | (3-6) |\n\n其中![img](6-1-my-paper/wps35.jpg)和![img](6-1-my-paper/wps36.jpg)表示两个图的注意权重，W1∈Rd’×2d这是可训练的权重矩阵，[·]是连接操作。之后，用以式(3-7)对注意力权重进行归一化处理：\n\n|      | ![img](6-1-my-paper/wps37.jpg) | (3-7) |\n| ---- | :----------------------------: | ----: |\n|      | ![img](6-1-my-paper/wps38.jpg) | (3-8) |\n\n其中sum（·）表示注意力权重的和，那么以同理，可以得到![img](6-1-my-paper/wps39.jpg)。因为![img](6-1-my-paper/wps40.jpg)+![img](6-1-my-paper/wps41.jpg) = 1，更大![img](6-1-my-paper/wps42.jpg)意味着社交关系网络对模型结果的影响占比更大。当然，每一层的注意值是按照当时的嵌入计算出，实现了自动优化权重。\n\n注意力层最终输出的用户嵌入为：\n\n|      | ![img](6-1-my-paper/wps43.jpg) | (3-9) |\n| ---- | :----------------------------: | ----: |\n\n## 3.4 预测推荐层\n\n对于模型输出的结果用聚合器聚合用户和项目的表示，可以得到一阶和高阶语义表示对项目关联性和用户偏好的影响。计算用户和项目向量内积，预测用户偏好，实现推荐。常用的聚合方式为加和聚合器和连接聚会器。\n\n加和聚合器：表示对多个不同层向量的加和（K为网络层数）：\n\n|      | ![img](6-1-my-paper/wps44.jpg) | (3-10) |\n| ---- | :----------------------------: | -----: |\n|      | ![img](6-1-my-paper/wps45.jpg) | (3-11) |\n\n连接聚合器：表示将多个不同层向量连接起来（K为网络层数）：\n\n|      | ![img](6-1-my-paper/wps46.jpg) | (3-12) |\n| ---- | :----------------------------: | -----: |\n|      | ![img](6-1-my-paper/wps47.jpg) | (3-13) |\n\n最后用![img](6-1-my-paper/wps48.jpg)、![img](6-1-my-paper/wps49.jpg)分别表示用户、物品的聚合后的表示矩阵，将两个聚合表示矩阵进行矩阵相乘，获得用户对项目的偏好预测值，如图3-8：\n\n![img](6-1-my-paper/wps50.jpg) \n\n图 3-8嵌入相乘预测推荐\n\n|      | ![img](6-1-my-paper/wps51.jpg) | (3-14) |\n| ---- | :----------------------------: | -----: |\n\n## 3.5 损失函数\n\n为了关注用户的隐式反馈，本模型选择了BPR 损失函数进行优化：\n\n|      | ![img](6-1-my-paper/wps52.png) | (3-15) |\n| ---- | :----------------------------: | -----: |\n\n其中![img](6-1-my-paper/wps53.jpg)是一个sigmoid 型函数。Θ = [Θ1, Θ2]，带有Θ1 = [P, Q]，和Θ2 =[F,![img](6-1-my-paper/wps54.jpg)]。λ是一个正则化参数，控制用户和项目自由嵌入的复杂性矩阵。Da= {(i, j)|i∈Ra∧j∈V−Ra }表示a与Ra的两两训练数据表示一个积极显示反馈的项目集。\n\n \n\n \n\n \n\n# 4  实验设置与结果评估\n\n在这一章节中，本文将利用公开数据集LastFM、Yelp 和Douban-book模拟一种真实的推荐场景来具体研究以下三个问题： 1）本课题模型与其他推荐模型（DiffNet[39]、DiffNet++[40]和LightGCN[11] ）相比效率；2）不同参数（模型层数、聚合器、嵌入维度）的设置将如何影响最终的推荐结果；3）对本模型进行注意力机制消融实验将如何影响最终的推荐结果。\n\n## 4.1实验环境\n\n4.1.1硬件环境\n\n1）处理器：Intel(R)Core(TM)i7-8565UCPU@1.80GHz1.99GHz\n\n2）显卡：NVIDIAGeForceMX150\n\n3）机带RAM：8.00GB(7.85GB可用)\n\n4.1.2软件环境\n\n1）Windows11家庭中文版（版本：22H2）\n\n2）Python3.9.10\n\n3）PyCharmCommunityEdition2022.1.164位\n\n4）Torch1.12.1\n\n5）Torchvision0.14.1+cu116\n\n6）Numpy1.21.6\n\n7）Scikit-Learn1.0.2\n\n## 4.2数据集\n\n本文选择在三个数据集上进行了实验：LastFM、Yelp 和Douban-book。LastFM是一个音乐社交平台，用户可通过它来发现新音乐、跟踪已知艺人，并与其他音乐爱好者进行互动。Yelp是一个跨平台的用户评论网站，主要提供有关当地商家的信息、用户评论和排名，旨在帮助用户更方便地寻找商家和服务。Douban-book是一个在线书籍社区，用户可以在其中记录自己读过的书、给书评分、撰写书评、参加书籍讨论等。其中Douban-book由 1-5 级的评分组成。 将豆瓣书中的评分小于或等于3，并将其余设置为1，表示隐式反馈数据。 表 4-1 总结了数据集的特征。 没有交互的用户在社交网络中被过滤掉。\n\n表 4-1 实验三个数据集的特征\n\n|             | 用户数 | 项目数 | 交互数 | 关系数 |  密度  |\n| :---------: | :----: | :----: | :----: | :----: | :----: |\n|   Last.fm   |  1892  | 17632  | 92834  | 25434  | 0.288% |\n|    Yelp     | 17237  | 38342  | 204448 | 143765 | 0.033% |\n| Douban-book | 13024  | 22347  | 792062 | 169150 | 0.272% |\n\n## 4.3评估指标\n\n### 4.3.1召回率(Recall)\n\n召回率(Recall)是衡量推荐算法召回用户历史兴趣的能力的一种重要指标。具体而言，召回率是指在所有用户实际感兴趣的物品中，推荐系统成功推荐的物品占比。在模型预测结果中，设置TP(True Positive) : 表示样本的真实类别为正,最后预测得到的结果也为正，FN(False Negative): 表示样本的真实类别为正,最后预测得到的结果为负，Recall的公式如下：\n\n|      | ![img](6-1-my-paper/wps55.jpg) | (4-1) |\n| ---- | :----------------------------: | ----: |\n\nRecall@K表示结果列表前K项的Recall，随着K的变化，可以得到一系列的Recall值。Recall@K的公式如下：\n\n|      | ![img](6-1-my-paper/wps56.jpg) | (4-2) |\n| ---- | :----------------------------: | ----: |\n\n其中TP@K指的是在推荐结果的前K个物品中，与用户兴趣相关的物品数量，即真正例的数量。FN@K指的是所有与用户兴趣相关的物品数量减去在前K个推荐结果中与用户兴趣相关的物品数量。\n\n### 4.3.2归一化折损累计增益(NDCG)\n\n归一化折损累计增益(NDCG)是衡量推荐系统中排序质量的一种指标。NDCG是DCG（折损累计增益）的归一化处理，用来衡量推荐算法排序算法对于用户兴趣的预测质量。用户u上的nDCG![img](6-1-my-paper/wps57.jpg)为：\n\n|      | ![img](6-1-my-paper/wps58.jpg) | (4-3) |\n| ---- | :----------------------------: | ----: |\n\n其中用户u的DCG![img](6-1-my-paper/wps59.jpg)的计算公式为：\n\n|      | ![img](6-1-my-paper/wps60.jpg) | (4-4) |\n| ---- | :----------------------------: | ----: |\n\n其中reli表示处于位置i的推荐结果的相关性，K表示要考察的推荐列表的大小。式（4-3）中用户u的IDCG为理想的DCG的计算公式为：\n\n|      | ![img](6-1-my-paper/wps61.jpg) | (4-5) |\n| ---- | :----------------------------: | ----: |\n\n## 4.4其他对比模型\n\nDiffNet[39]是一种用于推荐系统的深度神经网络模型，其灵感来源于传统的协同过滤推荐方法。它利用图形卷积神经网络来学习用户-项目（或者用户-用户）之间的相似性，其主要思想是通过学习一个项目（或用户）嵌入向量，然后在项目（用户）图上进行卷积操作来计算项目（或用户）之间的相似性，并将其用于推荐系统中。\n\nDiffNet++[40]是DiffNet[39]的改进版本，它在DiffNet[39]的基础上增加了时间因素和用户的行为模式。\n\nLightGCN[11] 是一种基于图形嵌入的轻量级推荐算法模型，是利用图嵌入技术从推荐中提取用户和物品之间的相似性，从而为用户提供个性化推荐列表。\n\n## 4.5实验设置\n\n​    在实验过程中，本课题将数据集训练集、测试集和验证集按照8:1:1划分。在对比实验方面，本课题使用召回率(Recall)、归一化折损累计增益(NDCG) 来评估个方法的预测效果。在参数影响实验方面，本课题控制变量对模型层数、聚合器种类、嵌入维度和消融注意力机制实验进行测试，探究各参数对实验结果的影响并进行分析。\n\n## 4.6对照实验结果\n\n本论文提出的模型在LastFM、Yelp 和Douban-book三个数据集下的Recall@10值和NDCG@10值方面都表现出了领先的结果，如表4-2。\n\n表 4-2 对照实验结果\n\n|           | Last.fm   |         | Yelp      |         | Douban-book |         |\n| --------- | --------- | ------- | --------- | ------- | ----------- | ------- |\n|           | Recall@10 | NDCG@10 | Recall@10 | NDCG@10 | Recall@10   | NDCG@10 |\n| 本模型    | 0.4434    | 0.3796  | 0.3853    | 0.3134  | 0.4012      | 0.3387  |\n| DiffNet   | 0.4216    | 0.3598  | 0.3687    | 0.2873  | 0.3919      | 0.3024  |\n| DiffNet++ | 0.4323    | 0.3748  | 0.3808    | 0.3022  | 0.4005      | 0.3293  |\n| LightGCN  | 0.4298    | 0.3539  | 0.3729    | 0.2885  | 0.3885      | 0.3124  |\n\n在Yelp数据集中，本论文模型的Recall@10值显著高于DiffNet[39]、DiffNet++[40]和LightGCN[11] 。具体而言，本论文模型的Recall@10值为0.4434，而DiffNet[39]、DiffNet++[40]和LightGCN[11] 的Recal@10值分别为0.4216、0.4323和0.4298。此外，本论文模型在Yelp数据集中的NDCG@10值也达到了极高水平，为0.3796，远高于其他三个模型。\n\n在Flickr数据集中，本论文模型的表现同样突出。本论文模型的Recall@10值为0.3853，远高于DiffNet[39]、DiffNet++[40]和LightGCN[11] 的值，分别为0.3687、0.3808和0.3729。此外，本论文模型的NDCG@10值也得到了显著提升，为0.3134。\n\n在Last.fm数据集中，本论文模型的表现也相当出色。本论文模型的Recall@10值为0.3387，略高于DiffNet[39]、DiffNet++[40]和LightGCN[11] 的Recal@10l值，为0.4012。与此同时，本论文模型在Last.fm数据集中的NDCG@10值也达到了0.3387，远高于其他三个模型。\n\n综合以上结果，本论文提出的模型在三个数据集下的表现都达到了较高水平，具有显著的优势和潜力，值得在实际推荐场景中进一步探索和应用，如图4-1。\n\n|   ![img](6-1-my-paper/wps62.jpg) (a) Last.fm数据集对比实验   |\n| :----------------------------------------------------------: |\n|   ![img](6-1-my-paper/wps63.jpg) (b)  Yelp数据集的对比实验   |\n| ![img](6-1-my-paper/wps64.jpg) (c) Douban-book数据集的对比实验 |\n\n图 4-1 模型在不同top-K推荐下的召回率\n\n## 4.7参照对实验结果分析影响\n\n本文还分析了网络层数、嵌入维度等参数以及不同聚合器对模型性能的影响，下面主要给出了在Yelp数据集中的实验情况。\n\n### 4.7.1模型网络层数对实验结果的影响\n\n在社交推荐模型中，网络层数通常会影响实验结果。当设置不同的网络层数模型预测结果如图4-2所示。显然，网络层数的适当设定可以提高模型的效果，但如果网络层数过多，模型可能会出现过拟合现象。\n\n![img](6-1-my-paper/wps65.jpg) \n\n图 4-2 不同网络层数模型的召回率\n\n### 4.7.2嵌入维度对实验结果的影响\n\n嵌入维度是推荐系统中一个非常重要的超参数，它可以影响到推荐系统实验的结果。现为了探究该参数对于本模型的预测结果的影响，改变模型向量的嵌入深度的大小分别为8，16，32，64和128，观察嵌入深度为这些值的时Recall@10的大小。实验结果如图4-3所示，由图可知当嵌入维度为64时，模型的预测结果最佳。\n\n \n\n![img](6-1-my-paper/wps66.jpg) \n\n图 4-3 不同嵌入维度模型的召回率\n\n### 4.7.3聚合器对实验结果的影响\n\n本文3.4中提到的，聚合器的两种主要的实现方式亦会对模型性能产生影响。现改变聚合器的的种类，探究不同聚合器对实验结果的影响。如图4-4两种聚合器中，加和聚合器对于本模型预测结果优于拼接聚合器。\n\n![img](6-1-my-paper/wps67.jpg) \n\n图 4-4 不同聚合器模型的召回率\n\n### 4.7.4注意力机制的消融实验\n\n为了进一步说明注意力机制在模型中的重要性或作用，通过对比有无注意力机制的模型实验结果，评估注意力机制对推荐算法性能的影响。本实验在实现的推荐算法中，分别使用了带有注意力机制的模型和不带注意力机制的基准模型进行训练和测试。根据Recall@10、NDCG@10进行评估，看两个模型差异的显著性。消融实验结果如图4-5。\n\n显然，模型中加入注意力机制可以显著提高模型预测的性能。\n\n![img](6-1-my-paper/wps68.jpg) \n\n图4-5 注意力机制的消融实验结果\n\n \n\n\n\n \n\n \n\n# 结论\n\n \n\n本篇论文通过研究并掌握多种融合注意力机制和图神经网络的社交推荐模型，结合图神经网络技术对传统协同过滤算法中的冷启动及稀疏性问题进行深入探究，提出了一种基于图神经网络和注意力机制的社交推荐算法，通过对用户历史交互行为进行分析，利用获取的用户及项目特征融合注意力机制进行推荐，并利用LastFM、Yelp 和Douban-book的真实数据集进行了模拟测试与比较。本算法相比于现有的推荐算法(DiffNet[39])存在着较大的优势，即提高了推荐的准确性和召回率。\n\n但是，本研究也存在一些不足，例如该算法还存在推荐瓶颈问题，对于大型数据集本模型运算通常需要大量的计算和存储资源。未来将探索更加高效的算法和大规模分布式计算技术等。此外，当前该算法只考虑了静态数据集，未来希望将时序信息和多模态数据纳入模型中，进一步提高推荐算法的效果。\n\n综上所述，本文致力于研究并构建了基于图神经网络和注意力机制的社交推荐算法，通过实验证明其在推荐准确性和召回率方面的有效性。其中所提出的思路及相应的技术手段也有望涉及到其他领域的研究方向。最后，相信随着未来研究的深入，在更多的场景下可以更好地利用图神经网络和注意力机制提高推荐效果，为社交推荐算法的发展做出更大的贡献。\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n# 致谢\n\n \n\n在完成本篇论文期间，我收获了许多宝贵的经历和收获，这些经验和收获离不开许多人的帮助和支持。因此，我要特别感谢以下两位尤其对本研究的支持和帮助：\n\n首先，我要向我的毕设导师孟顺梅老师表达最深切的谢意。孟老师一直以来都是我学术上的指导和榜样。她一直在培养我在研究中学习技能，并支持我进行独立的学术研究。她的鼓励、支持和指导是让我选择本课题的重要原因，没有她的支持，我将无法完成这篇论文。我要特别感谢孟老师对我的悉心指导和无私支持。\n\n其次，我要向庄嘉博学长表达我的感谢之情。他在我完成本论文期间提供了无私的帮助和支持。在我遇到困难和难题时，他总是耐心地听我倾诉，并给我提供最及时和最准确的建议和指导。他的经验和专业知识非常之丰富，帮助我理解研究中的许多难点。在本论文的研究过程中，他对我学术上的支持是至关重要的，我再次感谢他的无私帮助。\n\n最后，我还想感谢家人和朋友对我在完成整个论文过程中的支持，他们是我不懈努力的动力源泉。感谢热心的同学在本科期间给予的帮助和支持。我也要感谢论文中的研究对象和数据源，没有它们的支持，我的研究将无从谈起。\n\n在此，我再次向孟顺梅老师和庄嘉博学长表达我的最深刻的感谢之情，他们的支持和帮助是我研究成果的重要功臣。\n\n \n\n \n\n\n\n \n\n \n\n# 参考文献\n\n \n\n[1] Silva N B, Tsang R, Cavalcanti G D C, et al. A graph-based friend recommendation system using genetic algorithm[C]//IEEE congress on evolutionary computation. IEEE, 2010: 1-7.\n\n[2] Hamilton W, Ying Z, Leskovec J. Inductive representation learning on large graphs[J]. Advances in neural information processing systems, 2017, 30. \n\n[3] Ji S, Pan S, Cambria E, et al. A survey on knowledge graphs: Representation, acquisition, and applications[J]. IEEE transactions on neural networks and learning systems, 2021, 33(2): 494-514.\n\n[4] Shams B, Haratizadeh S. Graph-based collaborative ranking[J]. Expert Systems with Applications, 2017, 67: 59-70.\n\n[5] Shi C, Kong X, Huang Y, et al. Hetesim: A general framework for relevance measure in heterogeneous networks[J]. IEEE Transactions on Knowledge and Data Engineering, 2014, 26(10): 2479-2492.\n\n[6] Musto C, Basile P, Lops P, et al. Introducing linked open data in graph-based recommender systems[J]. Information Processing & Management, 2017, 53(2): 405-435.\n\n[7] Palumbo E, Rizzo G, Troncy R, et al. Knowledge graph embeddings with node2vec for item recommendation[C]//The Semantic Web: ESWC 2018 Satellite Events: ESWC 2018 Satellite Events, Heraklion, Crete, Greece, June 3-7, 2018, Revised Selected Papers 15. Springer International Publishing, 2018: 117-120.\n\n[8] Fan W, Ma Y, Li Q, et al. Graph neural networks for social recommendation[C]//The world wide web conference. 2019: 417-426.\n\n[9] Gao Y, Li Y F, Lin Y, et al. Deep learning on knowledge graph for recommender system: A survey[J]. arXiv preprint arXiv:2004.00387, 2020.\n\n[10] Ying R, He R, Chen K, et al. Graph convolutional neural networks for web-scale recommender systems[C]//Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 2018: 974-983.\n\n[11] He X, Deng K, Wang X, et al. Lightgcn: Simplifying and powering graph convolution network for recommendation[C]//Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 2020: 639-648.\n\n[12] Monti F, Boscaini D, Masci J, et al. Geometric deep learning on graphs and manifolds using mixture model cnns[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 5115-5124. \n\n[13] Defferrard M, Bresson X, Vandergheynst P. Convolutional neural networks on graphs with fast localized spectral filtering[J]. Advances in neural information processing systems, 2016, 29.\n\n[14] Berg R, Kipf T N, Welling M. Graph convolutional matrix completion[J]. arXiv preprint arXiv:1706.02263, 2017.\n\n[15] Fan W, Ma Y, Li Q, et al. Graph neural networks for social recommendation[C]//The world wide web conference. 2019: 417-426.\n\n[16] Zhang M, Chen Y. Inductive matrix completion based on graph neural networks[J]. arXiv preprint arXiv:1904.12058, 2019.\n\n[17] Sun J, Zhang Y, Ma C, et al. Multi-graph convolution collaborative filtering[C]//2019 IEEE international conference on data mining (ICDM). IEEE, 2019: 1306-1311. \n\n[18] Wang X, Jin H, Zhang A, et al. Disentangled graph collaborative filtering[C]//Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval. 2020: 1001-1010.\n\n[19] Zhong T, Zhang S, Zhou F, et al. Hybrid graph convolutional networks with multi-head attention for location recommendation[J]. World Wide Web, 2020, 23: 3125-3151.\n\n[20] Hekmatfar T, Haratizadeh S, Goliaei S. Embedding ranking-oriented recommender system graphs[J]. Expert Systems with Applications, 2021, 181: 115108.\n\n[21] Duran P G, Karatzoglou A, Vitria J, et al. Graph convolutional embeddings for recommender systems[J]. IEEE Access, 2021, 9: 100173-100184.\n\n[22] Song J, Chang C, Sun F, et al. Graph attention collaborative similarity embedding for recommender system[C]//Database Systems for Advanced Applications: 26th International Conference, DASFAA 2021, Taipei, Taiwan, April 11–14, 2021, Proceedings, Part III 26. Springer International Publishing, 2021: 165-178.\n\n[23] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.\n\n[24] Veličković P, Cucurull G, Casanova A, et al. Graph attention networks[J]. arXiv preprint arXiv:1710.10903, 2017.\n\n[25] Chen F, Pan S, Jiang J, et al. DAGCN: dual attention graph convolutional networks[C]//2019 International Joint Conference on Neural Networks (IJCNN). IEEE, 2019: 1-8.\n\n[26] Hong H, Guo H, Lin Y, et al. An attention-based graph neural network for heterogeneous structural learning[C]//Proceedings of the AAAI conference on artificial intelligence. 2020, 34(04): 4132-4139.\n\n[27] Zhang Z, Zhuang F, Zhu H, et al. Relational graph neural network with hierarchical attention for knowledge graph completion[C]//Proceedings of the AAAI conference on artificial intelligence. 2020, 34(05): 9612-9619.  \n\n[28] Chang H, Rong Y, Xu T, et al. Spectral graph attention network with fast eigen-approximation[C]//Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 2021: 2905-2909.\n\n[29] Wu Q, Zhang H, Gao X, et al. Dual graph attention networks for deep latent representation of multifaceted social effects in recommender systems[C]//The world wide web conference. 2019: 2091-2102.\n\n[30] Ashish V. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30: I.\n\n[31] Cheng Z, Ding Y, He X, et al. A^ 3NCF: An Adaptive Aspect Attention Model for Rating Prediction[C]//IJCAI. 2018: 3748-3754.[32] PEI W J，YANG J，SUN Z，et al. Interacting attention-gated recurrent networks for recommendation[C]  //Proceedings of the 2017 ACM on Conference on Information and Knowledge Management，Singapore， Nov 6-Nov 10，2017. New York: ACM，2017: 1459–1468. \n\n[33] 柴玉梅, 员武莲, 王黎明, 等. 基于双注意力机制和迁移学习的跨领域推荐模型[J]. 计算机学报, 2020, 43(10): 1924-1942.\n\n[34] Chen J, Zhang H, He X, et al. Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention[C]//Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. 2017: 335-344. \n\n[35] Guo X, Zhu J H. Deep Neural Network Recommendation ModelBasedon UserVectorizationRepresentationand AttenG tion Mechanism[J]. ComputerScience, 2019, 46(8): 111G115.\n\n[36] Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907, 2016.\n\n[37] Maas A L, Hannun A Y, Ng A Y. Rectifier nonlinearities improve neural network acoustic models[C]//Proc. icml. 2013, 30(1): 3.\n\n[38] 周飞燕, 金林鹏, 董军. 卷积神经网络研究综述[J]. 计算机学报, 2017, 40(6): 1229-1251.\n\n[39] Wu L, Sun P, Fu Y, et al. A neural influence diffusion model for social recommendation[C]//Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval. 2019: 235-244.\n\n[40] Wu L, Li J, Sun P, et al. Diffnet++: A neural influence and interest diffusion network for social recommendation[J]. IEEE Transactions on Knowledge and Data Engineering, 2020, 34(10): 4753-4766.\n\n \n","tags":["private"],"categories":["paper"]},{"title":"创建新的Hexo-Blog主题构想","url":"/2023/05/19/5-19-博客主题构想/","content":"\n- [ HOME](#head1)\n- [ BLOG](#head2)\n- [ RESUME](#head3)\n- [ HOBBY](#head4)\n- [ DIARY](#head5)\n- [ REVIEW](#head6)\n- [ PREVIEW](#head7)\n\n# <span id=\"head1\"> HOME</span>\n\n![img.png](5-19-博客主题构想/img.png)\n\n- 可以到达六个模块\n- 左上角可以转换四种语言\n- 右上角连接到GitHub账号\n\n# <span id=\"head2\"> BLOG</span>\n\n![img_1.png](5-19-博客主题构想/img_1.png)\n\n- 左部：按照类别和目录归纳，左上角是BLOG首页\n- 中部：博客内容，左上角是类别，右上角是修改时间，底部是评论区\n- 右部：文章toc，右上角是HOME页链接\n\n# <span id=\"head3\"> RESUME</span>\n\n![img_2.png](5-19-博客主题构想/img_2.png)\n\n- 左部：左上角是RESUME首页，左下角转换四种语言\n- 中部：简历\n- 右部：简历toc，右上角是HOME页链接\n\n# <span id=\"head4\"> HOBBY</span>\n\n![img_3.png](5-19-博客主题构想/img_3.png)\n\n- 左部：按照类别和目录归纳，左上角是HOBBY首页\n- 中部：内容，左上角是类别，右上角是时间戳\n- 右部：内容toc，右上角是HOME页链接\n\n# <span id=\"head5\"> DIARY</span>\n\n![img_4.png](5-19-博客主题构想/img_4.png)\n\n- 左部：日期+题目，左上角是DIARY首页\n- 中部：内容，左上角是今日心情、新日天气，右上角是时间戳\n- 右部：本月心情记录，本月天气记录，右上角是HOME页链接\n\n# <span id=\"head6\"> REVIEW</span>\n![img_5.png](5-19-博客主题构想/img_5.png)\n\n\n- 左部：年、月、日归纳，点击年有年复盘，点击月有月复盘，点击日是日复盘，左上角是REVIEW首页\n- 中部：\n  - 日复盘：按日复盘，左上角是第几周周几，右上角是时间戳\n  - 月复盘：按月复盘，左上角是几月，右上角是时间戳\n  - 年复盘：按年复盘，左上角是几年，右上角是时间戳\n- 右部：在日历上，按照周次和周几复盘，点击第几周是周复盘，右上角是HOME页链接\n\n# <span id=\"head7\"> PREVIEW</span>\n\n![img_6.png](5-19-博客主题构想/img_6.png)\n\n- 左部：按照规划类型归纳，左上角是PREVIEW首页\n- 中部：\n- 右部：内容toc，右上角是HOME页链接\n\n","tags":["hobby","private"],"categories":["thoughts"]},{"title":"Array 3:Squares of a Sorted Array","url":"/2023/05/14/5-14-array-3-squares-of-a-sorted-array/","content":"- [977. 有序数组的平方](#head1)\n    - [ NOTE](#head2)\n    - [c++ solution](#head3)\n    - [java solution](#head4)\n    - [improved solution](#head5)\n\n# <span id=\"head1\">[977. 有序数组的平方](https://leetcode.cn/problems/squares-of-a-sorted-array/description/)</span>\n\n![977](5-14-array-3-squares-of-a-sorted-array/img.png)\n\n## <span id=\"head2\"> NOTE</span>\n\n原本自己读完题想着要先挨个平方完，然后快排（没错本人就是这么垃圾:sob:）这样的话，是复杂度是O(n+nlogn)，\n绝对超过了O(n)。之后看到了提示说用双指针，就恍然大悟：①遍历数组找到正负分界点②从这个点向两端依次确定\n最小值，直到有一侧达到尽头③处理剩下的数组。写完之后，发现卡哥的思路：从数组两端向中间聚拢，以此确定最大值，\n这样省了找分界点和处理剩下元素的时间，respect！！\n\n## <span id=\"head3\">c++ solution</span>\n\n```c++\nclass Solution {\npublic:\n    vector<int> sortedSquares(vector<int>& nums) {\n        vector<int> ans(nums.size());\n        int FLAG=1;\n        int r=0;\n        while(nums[r]<0){\n            r++;\n            if(r>=nums.size())break;\n        }\n        int l=r-1;\n        if(l<0||r>=nums.size())FLAG=0;\n        int idx=0;\n        while(FLAG&&idx<nums.size()){\n            if(nums[l]*(-1)<nums[r]){\n                ans[idx]=nums[l]*nums[l];\n                l--;\n                if(l<0)FLAG=0;\n            }else{\n                ans[idx]=nums[r]*nums[r];\n                r++;\n                if(r>=nums.size())FLAG=0;\n            }\n            idx++;\n        }\n\n        if(FLAG==0){\n            if(l<0){//剩下右边\n                while(idx<nums.size()&&r<nums.size()){\n                    ans[idx]=nums[r]*nums[r];\n                    idx++;\n                    r++;\n                }\n            }else{\n                 while(idx<nums.size()&&l>=0){\n                    ans[idx]=nums[l]*nums[l];\n                    idx++;\n                    l--;\n                }\n            }\n        }\n\n        return ans;\n\n    }\n};\n\n```\n## <span id=\"head4\">java solution</span>\n\n```java\nclass Solution {\n    public int[] sortedSquares(int[] nums) {\n        \n        int len=nums.length;\n        int[]ans=new int[len];\n        int FLAG=1;\n\n        int r=0;\n        while(nums[r]<0){\n            r++;\n            if(r>=len){\n                FLAG=0;\n                break;\n            }\n        }\n\n        int l=r-1;\n        if(l<0) FLAG=0;\n\n        int idx=0;\n        while(FLAG==1&&idx<len){\n\n            if(nums[l]*(-1)<nums[r]){\n                ans[idx]=nums[l]*nums[l];\n                l--;\n                if(l<0)FLAG=0;\n            }else{\n                ans[idx]=nums[r]*nums[r];\n                r++;\n                if(r>=len)FLAG=0;\n            }\n\n            idx++;\n        }\n\n        if(FLAG==0){\n            if(l<0){//只剩右边\n\n                while(idx<len&&r<len){\n                    ans[idx]=nums[r]*nums[r];\n                    idx++;\n                    r++;\n                }\n                \n            }else{\n                while(idx<len&&l>=0){\n                    ans[idx]=nums[l]*nums[l];\n                    idx++;\n                    l--;\n                }\n            }\n        }\n        return ans;\n    }\n}\n```\n\n## <span id=\"head5\">improved solution</span>\n\n显然代码短了很多\n\n```c++\nclass Solution{\n\n public:\n\n    vector<int> sortedSquares(vector<int>& nums){\n        int len = nums.size();\n        vector<int>ans(len);\n\n        int l=0;\n        int r=len-1;\n        int idx=len-1;\n\n        while(idx>=0){\n            if(nums[l]*nums[l]>nums[r]*nums[r]){\n                ans[idx]=nums[l]*nums[l];\n                l++;\n            }else{\n                ans[idx]=nums[r]*nums[r];\n                r--;\n            }\n            idx--;\n        }\n\n        return ans;\n    }\n\n};\n\n```","tags":["blog"],"categories":["algorithm-training"]},{"title":"Array 2:Remove Element","url":"/2023/05/11/5-11-array-2-remove-element/","content":"\n- [27. 移除元素](#head1)\n    - [C++ solution](#head2)\n    - [Java solution](#head3)\n- [26. 删除有序数组中的重复项](#head4)\n    - [C++ solution](#head5)\n    - [Java solution](#head6)\n\n\n\n# <span id=\"head1\">[27. 移除元素](https://leetcode.cn/problems/remove-element/)</span>\n\n![27](5-11-array-2-remove-element/1.png)\n\n## <span id=\"head2\">C++ solution</span>\n\n```c++\nclass Solution {\npublic:\n    int removeElement(vector<int>& nums, int val) {\n\n       int cur_index =0;\n       int length=nums.size();\n       int last_index=length-1;\n\n       if(length==1&&nums[0]==val)return 0;\n\n       while(cur_index<length){\n           if(nums[cur_index]==val){\n               while(nums[last_index]==val){\n                   length--;\n                   if(length==0)return 0;\n                    last_index=length-1;\n                    \n               }\n               if(cur_index<last_index){\n                    int t=nums[last_index];\n                    nums[last_index]=nums[cur_index];\n                    nums[cur_index]=t;\n               }\n               \n           }\n           cur_index++;\n       }\n         \n\t\treturn length;\n    }\n};\n```\n\n## <span id=\"head3\">Java solution</span>\n\n```java\n    class Solution {\n    public int removeElement(int[] nums, int val) {\n      \n      int cur=0;\n      int length=nums.length;\n      int last=length-1;\n\n      if(length==0)return 0;\n\n      while(cur<length){\n\n          if(nums[cur]==val){\n\n              while(nums[last]==val){\n                    length--;\n                    if(length==0)return 0;\n                    last=length-1;\n              }\n              if(cur<last){\n                  int t=nums[cur];\n                  nums[cur]=nums[last];\n                  nums[last]=t;\n              }\n          }\n          cur++;\n      }\n\n        return length;\n    }\n\n}\n```\n\n#  class Solution{​ public:    vector<int> sortedSquares(vector<int>& nums){        int len = nums.size();        vector<int>ans(len);​        int l=0;        int r=len-1;        int idx=len-1;​        while(idx>=0){            if(nums[l]*nums[l]>nums[r]*nums[r]){                ans[idx]=nums[l]*nums[l];                l++;            }else{                ans[idx]=nums[r]*nums[r];                r--;            }            idx--;        }​        return ans;    }​};​c++\n\n![26](5-11-array-2-remove-element/2.png)\n\n## <span id=\"head5\">C++ solution</span>\n\n```c++\nclass Solution {\npublic:\n    int removeDuplicates(vector<int>& nums) {\n        int cur=0;\n        int next=cur+1;\n\n        if(nums.size()==1)return 1;\n\n        while(cur<=nums.size()-1){\n\n            while(next<nums.size()&&nums[next]==nums[cur])next++;\n            cur++;\n            if(cur>=nums.size()||next>=nums.size())break;\n\n            nums[cur]=nums[next];\n            \n            \n        }\n        return cur;\n    }\n    \n};\n```\n\n## <span id=\"head6\">Java solution</span>\n\n```java\nclass Solution {\n    public int removeDuplicates(int[] nums) {\n        int cur=0;\n        int next=cur+1;\n\n        if(nums.length==1)return 1;\n\n        while(cur<=nums.length-1){\n           \n            while(next<nums.length&&nums[next]==nums[cur])next++;\n            cur++;\n            if(cur==nums.length||next==nums.length)break;\n            nums[cur]=nums[next];\n\n        }\n\n        return cur;\n    }\n}\n```","tags":["blog"],"categories":["algorithm-training"]},{"title":"Array 1:Binary Search","url":"/2023/05/10/5-10-array-1-binary-search/","content":"\n[toc]\n\n\n# <span id=\"head1\"> __[Ⅰ 704 二分查找](https://leetcode.cn/problems/binary-search/)__ </span>\n\n\n![704](5-10-array-1-binary-search/1.png)\n\n## <span id=\"head2\"> NOTE</span>\n\n\n### <span id=\"head3\">1. 每次写二分最容易纠结的part就是下面C++代码里面的A,B,C,D行</span>\n- A 处是用nums.size()-1还是nums.size()?\n- B 处是 <= 还是 < ?\n- C 处是 mid 还是 mid - 1 ?\n- D 处是 mid 还是 mid + 1 ?\n\n实际上这些不同的本质实际上是对于数组区间的划分法不同：\n\n|     | 左闭右闭区间                   | 左闭右开区间                 |\n|-----|--------------------------|------------------------|\n| A   | ```high=nums.size()-1``` | ```high=nums.size()``` |\n| B   | ``` while(low<=high)```  | ``` while(low<high)``` |\n| C   | ``` high=mid-1;```       | ``` high=mid;```       |\n| D   | ```low=mid+1;```         | ```low=mid+1;```       |\n\n### <span id=\"head4\">2. 二分法取中间值防溢出</span>\n\n```c++\n mid = (low+high) / 2\n```\nE 处的写法在两大数相加的时候容易发生溢出，可以改成下面的写法：\n```c++\n mid = low + (( high - low ) / 2)\n```\n\n## <span id=\"head5\">C++ solution</span>\n```c++\nclass Solution {\npublic \n    int search(vector<int>& nums, int target) {\n\n/*A*/   int low=0,high=nums.size()-1;\n\n/*B*/   while(low<=high){\n\n/*E*/        int mid=(low+high)/2;\n\n            if(target==nums[mid]){\n                return mid;\n            }else if(target<nums[mid]){\n/*C*/           high=mid-1;\n            }else{\n/*D*/           low=mid+1;\n            }\n\n        }\n\n        return -1;\n    }\n};\n```\n## <span id=\"head6\">Java solution</span>\n```java\nclass Solution {\n    public int search(int[] nums, int target) {\n\n        int low=0,high=nums.length-1;\n\n        while(low<=high){\n\n            int mid=(low+high)/2;\n\n            if(nums[mid]==target){\n                return mid;\n            }else if(nums[mid]>=target){\n                high=mid-1;\n            }else{\n                low=mid+1;\n            }\n\n        }\n\n        return -1;\n\n    }\n}\n```\n\n\n# <span id=\"head7\">__[Ⅱ  35. 搜索插入位置](https://leetcode.cn/problems/search-insert-position/)__</span>\n\n![35](5-10-array-1-binary-search/2.png)\n\n## <span id=\"head8\">C++ solution</span>\n```c++\nclass Solution {\npublic:\n    int searchInsert(vector<int>& nums, int target) {\n        int low =0,high=nums.size()-1,mid,ans=nums.size();\n\n        while(low<=high){\n\n            mid =(low+high)>>1;\n\t\t\t\n            if(target<=nums[mid]){//在左边 \n            \tans=mid;\n                high=mid-1;\n            }else{\n                low=mid+1;\n            }\n        }\n        return ans;\n\n    }\n};\n```\n## <span id=\"head9\">Java solution</span>\n```java\n\nclass Solution {\n    public int searchInsert(int[] nums, int target) {\n\n        int low = 0, high = nums.length-1, ans = nums.length;\n\n        while(low<=high){\n\n            int mid = ( low + high ) >> 1;\n\n            if(target <= nums[mid]){// 在左边\n\n                ans = mid;\n\n                high = mid-1;\n\n            } else{\n\n                low = mid +1;\n\n            }\n        }\n         return ans;   \n    }\n}\n\n```\n\n# <span id=\"head10\">[ __Ⅲ 34. 在排序数组中查找元素的第一个和最后一个位置__](https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array/)</span>\n\n![34](5-10-array-1-binary-search/3.png)\n\n## <span id=\"head11\"> NOTE</span>\n\n### <span id=\"head12\">1. 使用二分法在有序数组中查找某个数的左边界，即某个数第一次出现的位置</span>\n\n\n\n![image-20230610130542299](5-10-array-1-binary-search/image-20230610130542299.png)\n\n\n\n### <span id=\"head13\">2. 使用二分法在有序数组中查找某个数的右边界，即某个数最后一次出现的位置</span>\n\n与查找左边界不同的一个关键点有计算mid的时候需要l+r+1！\n否则会陷入死循环\n\n\n![image-20230610130632156](5-10-array-1-binary-searchimage-20230610130632156.png)\n\n\n## <span id=\"head14\">C++ solution</span>\n\n```c++\n\nclass Solution {\npublic:\n    vector<int> searchRange(vector<int>& nums, int target) {\n        \n        int left = binarySearch(nums,target,true);\n        int right =  binarySearch(nums,target,false)-1;\n        \n        if(left <= right && right < nums.size() && nums[left] == target && nums[right] == target){\n        \t\n        \treturn vector<int>{left,right};\n\t\t}else{\n\t\t\treturn vector<int>{-1,-1};\n\t\t}\n        \n    }\n    \n    int binarySearch(vector<int>nums,int target,bool isLeft){\n    \tint low =0,high=nums.size()-1,mid,ans=nums.size();\n    \t\n    \twhile(low<=high){\n    \t\t\n    \t\tmid=(low+high)>>1;\n    \t\t\n    \t\tif(target<nums[mid]||isLeft&&target<=nums[mid]){\n    \t\t\tans = mid;\n    \t\t\thigh = mid - 1;\n\t\t\t\t}else{\t\t\t\t\n\t\t\t\tlow = mid + 1;\n\t\t\t}\n\t\t}\n    \treturn ans;\n\t}\n    \n};\n\n```\n\n## <span id=\"head15\">Java solution</span>\n```java\nclass Solution {\n    public int[] searchRange(int[] nums, int target) {\n      int [] arr={-1,-1};\n\t\t\tint left = binarySearch(nums, target, true);\n\t\t\tint right = binarySearch(nums, target, false)-1;\n\n\t\t\tif(left<=right && right<nums.length && nums[left] == target && nums[right] == target){\n\t\t\t\tarr[0]=left;\n\t\t\t\tarr[1]=right;\n\t\t\t} \n\t\t\treturn arr;\n\n    }\n\t\tpublic int binarySearch(int[] nums, int target, boolean isLeft){\n\t\t\tint low = 0, high = nums.length-1, mid, ans = nums.length;\n\t\t\twhile(low<=high){\n\t\t\t\tmid=(low+high)>>1;\n\t\t\t\tif(target < nums[mid]|| isLeft && target <= nums[mid]){\n\t\t\t\t\tans = mid;\n\t\t\t\t\thigh = mid - 1;\n\t\t\t\t}else{\n\t\t\t\t\tlow = mid + 1;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n}\n```","tags":["blog"],"categories":["algorithm-training"]}]